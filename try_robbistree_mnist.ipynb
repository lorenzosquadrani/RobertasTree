{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "try_robbistree_mnist.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "9qs3jYVOvdp3"
      ],
      "mount_file_id": "1bzKWbiqj6S8DQPbeaDf9CxiO74Vqf6P6",
      "authorship_tag": "ABX9TyN5oRYkZWT0bz/KbMM4npOK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lorenzosquadrani/RobertasTree/blob/main/try_robbistree_mnist.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T0ZBWYdsqHM9"
      },
      "source": [
        "# Install packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xk1QxdFj3aP6"
      },
      "source": [
        "! git clone https://github.com/lorenzosquadrani/RobertasTree.git --quiet"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wyAqG1j96Ooc",
        "outputId": "e9490995-d269-4da9-c0d4-fec076e425e5"
      },
      "source": [
        " cd RobertasTree/"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/RobertasTree\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h-Z7NKafq9AD"
      },
      "source": [
        "!python -m pip install -r requirements.txt --quiet"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sBUbKFle6R9e",
        "outputId": "0a4452c3-2d56-490c-82f8-624fdb6da6b8"
      },
      "source": [
        "!python setup.py develop --user"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "running develop\n",
            "running egg_info\n",
            "creating robertastree.egg-info\n",
            "writing robertastree.egg-info/PKG-INFO\n",
            "writing dependency_links to robertastree.egg-info/dependency_links.txt\n",
            "writing requirements to robertastree.egg-info/requires.txt\n",
            "writing top-level names to robertastree.egg-info/top_level.txt\n",
            "writing manifest file 'robertastree.egg-info/SOURCES.txt'\n",
            "writing manifest file 'robertastree.egg-info/SOURCES.txt'\n",
            "running build_ext\n",
            "Creating /root/.local/lib/python3.7/site-packages/robertastree.egg-link (link to .)\n",
            "Adding robertastree 0.1.0 to easy-install.pth file\n",
            "\n",
            "Installed /content/RobertasTree\n",
            "Processing dependencies for robertastree==0.1.0\n",
            "Searching for torch==1.9.0+cu102\n",
            "Best match: torch 1.9.0+cu102\n",
            "Adding torch 1.9.0+cu102 to easy-install.pth file\n",
            "Installing convert-caffe2-to-onnx script to /root/.local/bin\n",
            "Installing convert-onnx-to-caffe2 script to /root/.local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for pandas==1.1.5\n",
            "Best match: pandas 1.1.5\n",
            "Adding pandas 1.1.5 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for numpy==1.19.5\n",
            "Best match: numpy 1.19.5\n",
            "Adding numpy 1.19.5 to easy-install.pth file\n",
            "Installing f2py script to /root/.local/bin\n",
            "Installing f2py3 script to /root/.local/bin\n",
            "Installing f2py3.7 script to /root/.local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for typing-extensions==3.7.4.3\n",
            "Best match: typing-extensions 3.7.4.3\n",
            "Adding typing-extensions 3.7.4.3 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for python-dateutil==2.8.2\n",
            "Best match: python-dateutil 2.8.2\n",
            "Adding python-dateutil 2.8.2 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for pytz==2018.9\n",
            "Best match: pytz 2018.9\n",
            "Adding pytz 2018.9 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for six==1.15.0\n",
            "Best match: six 1.15.0\n",
            "Adding six 1.15.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Finished processing dependencies for robertastree==0.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3PmNp-D6qLjn"
      },
      "source": [
        "# RobertasTree on MNIST"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yIyDQ10B_dus"
      },
      "source": [
        "## Prepare dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2BUgagpHNnOJ"
      },
      "source": [
        "from sklearn.datasets import fetch_openml\n",
        "X, y = fetch_openml('mnist_784', version=1, return_X_y=True)\n",
        "y = y.astype('int')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4OKUfNXqiTjQ"
      },
      "source": [
        "X = X[y < 8]\n",
        "y = y[y < 8]"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pNRKjbGiQBEx"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "dataset = pd.DataFrame(X)\n",
        "dataset['label'] = pd.Series(y.astype('int'))"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sq5r5PqSAm-h"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train_valid_set, testset = train_test_split(dataset, test_size=0.2,random_state=42)\n",
        "trainset, validset = train_test_split(train_valid_set, test_size=0.2, random_state=42)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s5L8Htt6-5wf"
      },
      "source": [
        "## Pytorch classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q7CNw6Hw-pw_"
      },
      "source": [
        "import torch\n",
        "\n",
        "class SimpleClassifier(torch.nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(SimpleClassifier, self).__init__()\n",
        "        \n",
        "        self.linear1 = torch.nn.Linear(784, 16)\n",
        "        self.linear2 = torch.nn.Linear(16, num_classes)\n",
        "\n",
        "        self.dropout = torch.nn.Dropout(0.1)\n",
        "        self.relu = torch.nn.ReLU()\n",
        "\n",
        "        \n",
        "    def forward(self, x):\n",
        "        \n",
        "        out = self.relu(self.linear1(x))\n",
        "        out = self.linear2(self.dropout(out))\n",
        "\n",
        "        return out"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0lIrT7WKAGHf"
      },
      "source": [
        "classifier = SimpleClassifier(num_classes=2)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_2ogSM2RAA_f"
      },
      "source": [
        "## Build the tree"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3SbVqZeyI1xv"
      },
      "source": [
        "!mkdir ../data"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sw6164Mg-BiK",
        "outputId": "0970a719-c3b2-4e2a-db92-5a9338dd2270"
      },
      "source": [
        "from robertastree.model import Tree\n",
        "\n",
        "tree = Tree(classifier=classifier,\n",
        "            trainset=trainset,\n",
        "            validset=validset,\n",
        "            models_path = '../data')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found GPU Tesla K80. I will use it.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MvIx7EmncDFL"
      },
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "class SimpleDataset(Dataset):\n",
        "    def __init__(self, dataframe):\n",
        "        super(SimpleDataset,self).__init__()\n",
        "\n",
        "        self.inputs = dataframe.drop(['label',], axis=1)\n",
        "        self.labels = dataframe.label\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        sample = torch.tensor(self.inputs.iloc[idx].values, dtype = torch.float)\n",
        "        label = torch.tensor(self.labels.iloc[idx], dtype = torch.long)\n",
        "\n",
        "        return {'x':sample}, label\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jf3MMh8kLa2x"
      },
      "source": [
        "tree.configure(optimizer= torch.optim.SGD,\n",
        "               optimizer_params={'lr':2e-3,\n",
        "                                'weight_decay':1e-4},\n",
        "               loss_function=torch.nn.CrossEntropyLoss(),\n",
        "               dataset_class=SimpleDataset,\n",
        "               batch_size=128,\n",
        "               num_epochs=100,\n",
        "               scheduler=torch.optim.lr_scheduler.MultiStepLR,\n",
        "               scheduler_params={'milestones':(70,), 'gamma':0.1},\n",
        "               valid_period=100  #very big value, to validate only at the beginning of each epoch\n",
        "            ) "
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "40RVO-nB7O2T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec1e3bc8-30e6-40f9-d404-e60dfabf6b83"
      },
      "source": [
        "tree.train()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "========== Training classifier [0,0] ==========\n",
            "[epoch, batch/num_batches]: trainloss | validloss | best_validloss | accuracy\n",
            "[   0,  100/ 282]: 1.1863 | 0.2466 | 89.84% |   inf\n",
            "[   0,  200/ 282]: 0.7168 | 0.1909 | 91.83% | 0.2466\n",
            "[   1,   18/ 282]: 0.2236 | 0.1659 | 93.39% | 0.1909\n",
            "[   1,  118/ 282]: 0.2046 | 0.1529 | 94.33% | 0.1659\n",
            "[   1,  218/ 282]: 0.1981 | 0.1482 | 94.50% | 0.1529\n",
            "[   2,   36/ 282]: 0.1683 | 0.1310 | 95.15% | 0.1482\n",
            "[   2,  136/ 282]: 0.1748 | 0.1265 | 95.38% | 0.1310\n",
            "[   2,  236/ 282]: 0.1736 | 0.1272 | 95.56% | 0.1265\n",
            "[   3,   54/ 282]: 0.1577 | 0.1215 | 95.69% | 0.1265\n",
            "[   3,  154/ 282]: 0.1615 | 0.1185 | 95.64% | 0.1215\n",
            "[   3,  254/ 282]: 0.1597 | 0.1163 | 95.82% | 0.1185\n",
            "[   4,   72/ 282]: 0.1495 | 0.1111 | 96.12% | 0.1163\n",
            "[   4,  172/ 282]: 0.1492 | 0.1109 | 96.21% | 0.1111\n",
            "[   4,  272/ 282]: 0.1477 | 0.1025 | 96.50% | 0.1109\n",
            "[   5,   90/ 282]: 0.1411 | 0.0992 | 96.55% | 0.1025\n",
            "[   5,  190/ 282]: 0.1407 | 0.0971 | 96.66% | 0.0992\n",
            "[   6,    8/ 282]: 0.1185 | 0.0991 | 96.78% | 0.0971\n",
            "[   6,  108/ 282]: 0.1290 | 0.1002 | 96.60% | 0.0971\n",
            "[   6,  208/ 282]: 0.1329 | 0.0948 | 96.68% | 0.0971\n",
            "[   7,   26/ 282]: 0.1264 | 0.0915 | 96.93% | 0.0948\n",
            "[   7,  126/ 282]: 0.1276 | 0.0914 | 97.00% | 0.0915\n",
            "[   7,  226/ 282]: 0.1258 | 0.0908 | 97.07% | 0.0914\n",
            "[   8,   44/ 282]: 0.1138 | 0.0902 | 96.96% | 0.0908\n",
            "[   8,  144/ 282]: 0.1199 | 0.0893 | 97.08% | 0.0902\n",
            "[   8,  244/ 282]: 0.1271 | 0.0847 | 97.18% | 0.0893\n",
            "[   9,   62/ 282]: 0.1226 | 0.0876 | 97.21% | 0.0847\n",
            "[   9,  162/ 282]: 0.1205 | 0.0856 | 97.08% | 0.0847\n",
            "[   9,  262/ 282]: 0.1195 | 0.0844 | 97.18% | 0.0847\n",
            "[  10,   80/ 282]: 0.1120 | 0.0823 | 97.32% | 0.0844\n",
            "[  10,  180/ 282]: 0.1099 | 0.0817 | 97.27% | 0.0823\n",
            "[  10,  280/ 282]: 0.1144 | 0.0830 | 97.24% | 0.0817\n",
            "[  11,   98/ 282]: 0.1103 | 0.0808 | 97.34% | 0.0817\n",
            "[  11,  198/ 282]: 0.1116 | 0.0824 | 97.21% | 0.0808\n",
            "[  12,   16/ 282]: 0.1157 | 0.0836 | 97.21% | 0.0808\n",
            "[  12,  116/ 282]: 0.1077 | 0.0816 | 97.15% | 0.0808\n",
            "[  12,  216/ 282]: 0.1070 | 0.0820 | 97.31% | 0.0808\n",
            "[  13,   34/ 282]: 0.0932 | 0.0814 | 97.20% | 0.0808\n",
            "[  13,  134/ 282]: 0.1054 | 0.0812 | 97.28% | 0.0808\n",
            "[  13,  234/ 282]: 0.1056 | 0.0784 | 97.34% | 0.0808\n",
            "[  14,   52/ 282]: 0.1064 | 0.0823 | 97.24% | 0.0784\n",
            "[  14,  152/ 282]: 0.1023 | 0.0778 | 97.45% | 0.0784\n",
            "[  14,  252/ 282]: 0.1044 | 0.0784 | 97.44% | 0.0778\n",
            "[  15,   70/ 282]: 0.1020 | 0.0778 | 97.40% | 0.0778\n",
            "[  15,  170/ 282]: 0.1008 | 0.0755 | 97.54% | 0.0778\n",
            "[  15,  270/ 282]: 0.1019 | 0.0780 | 97.55% | 0.0755\n",
            "[  16,   88/ 282]: 0.1002 | 0.0778 | 97.52% | 0.0755\n",
            "[  16,  188/ 282]: 0.0984 | 0.0784 | 97.39% | 0.0755\n",
            "[  17,    6/ 282]: 0.0985 | 0.0776 | 97.45% | 0.0755\n",
            "[  17,  106/ 282]: 0.0978 | 0.0782 | 97.50% | 0.0755\n",
            "[  17,  206/ 282]: 0.0971 | 0.0772 | 97.47% | 0.0755\n",
            "[  18,   24/ 282]: 0.1076 | 0.0776 | 97.55% | 0.0755\n",
            "[  18,  124/ 282]: 0.0994 | 0.0762 | 97.53% | 0.0755\n",
            "[  18,  224/ 282]: 0.0970 | 0.0743 | 97.64% | 0.0755\n",
            "[  19,   42/ 282]: 0.0902 | 0.0750 | 97.49% | 0.0743\n",
            "[  19,  142/ 282]: 0.0932 | 0.0766 | 97.49% | 0.0743\n",
            "[  19,  242/ 282]: 0.0959 | 0.0755 | 97.47% | 0.0743\n",
            "[  20,   60/ 282]: 0.0944 | 0.0779 | 97.40% | 0.0743\n",
            "[  20,  160/ 282]: 0.0901 | 0.0762 | 97.48% | 0.0743\n",
            "[  20,  260/ 282]: 0.0903 | 0.0742 | 97.53% | 0.0743\n",
            "[  21,   78/ 282]: 0.0940 | 0.0729 | 97.52% | 0.0742\n",
            "[  21,  178/ 282]: 0.0926 | 0.0728 | 97.63% | 0.0729\n",
            "[  21,  278/ 282]: 0.0935 | 0.0738 | 97.53% | 0.0728\n",
            "[  22,   96/ 282]: 0.0906 | 0.0739 | 97.51% | 0.0728\n",
            "[  22,  196/ 282]: 0.0895 | 0.0743 | 97.55% | 0.0728\n",
            "[  23,   14/ 282]: 0.0799 | 0.0732 | 97.61% | 0.0728\n",
            "[  23,  114/ 282]: 0.0860 | 0.0770 | 97.58% | 0.0728\n",
            "[  23,  214/ 282]: 0.0901 | 0.0731 | 97.58% | 0.0728\n",
            "[  24,   32/ 282]: 0.0892 | 0.0731 | 97.61% | 0.0728\n",
            "[  24,  132/ 282]: 0.0870 | 0.0773 | 97.50% | 0.0728\n",
            "[  24,  232/ 282]: 0.0875 | 0.0750 | 97.70% | 0.0728\n",
            "[  25,   50/ 282]: 0.0856 | 0.0785 | 97.53% | 0.0728\n",
            "[  25,  150/ 282]: 0.0828 | 0.0763 | 97.55% | 0.0728\n",
            "[  25,  250/ 282]: 0.0857 | 0.0726 | 97.54% | 0.0728\n",
            "[  26,   68/ 282]: 0.0857 | 0.0724 | 97.63% | 0.0726\n",
            "[  26,  168/ 282]: 0.0831 | 0.0754 | 97.75% | 0.0724\n",
            "[  26,  268/ 282]: 0.0838 | 0.0725 | 97.63% | 0.0724\n",
            "[  27,   86/ 282]: 0.0842 | 0.0714 | 97.62% | 0.0724\n",
            "[  27,  186/ 282]: 0.0804 | 0.0722 | 97.68% | 0.0714\n",
            "[  28,    4/ 282]: 0.0728 | 0.0742 | 97.65% | 0.0714\n",
            "[  28,  104/ 282]: 0.0804 | 0.0761 | 97.51% | 0.0714\n",
            "[  28,  204/ 282]: 0.0821 | 0.0732 | 97.64% | 0.0714\n",
            "[  29,   22/ 282]: 0.0815 | 0.0742 | 97.61% | 0.0714\n",
            "[  29,  122/ 282]: 0.0778 | 0.0758 | 97.67% | 0.0714\n",
            "[  29,  222/ 282]: 0.0797 | 0.0706 | 97.73% | 0.0714\n",
            "[  30,   40/ 282]: 0.0774 | 0.0747 | 97.73% | 0.0706\n",
            "[  30,  140/ 282]: 0.0793 | 0.0708 | 97.68% | 0.0706\n",
            "[  30,  240/ 282]: 0.0815 | 0.0718 | 97.70% | 0.0706\n",
            "[  31,   58/ 282]: 0.0894 | 0.0749 | 97.67% | 0.0706\n",
            "[  31,  158/ 282]: 0.0848 | 0.0706 | 97.74% | 0.0706\n",
            "[  31,  258/ 282]: 0.0836 | 0.0698 | 97.74% | 0.0706\n",
            "[  32,   76/ 282]: 0.0834 | 0.0712 | 97.78% | 0.0698\n",
            "[  32,  176/ 282]: 0.0803 | 0.0707 | 97.79% | 0.0698\n",
            "[  32,  276/ 282]: 0.0793 | 0.0726 | 97.73% | 0.0698\n",
            "[  33,   94/ 282]: 0.0827 | 0.0686 | 97.74% | 0.0698\n",
            "[  33,  194/ 282]: 0.0812 | 0.0691 | 97.87% | 0.0686\n",
            "[  34,   12/ 282]: 0.0783 | 0.0738 | 97.84% | 0.0686\n",
            "[  34,  112/ 282]: 0.0734 | 0.0701 | 97.79% | 0.0686\n",
            "[  34,  212/ 282]: 0.0774 | 0.0693 | 97.75% | 0.0686\n",
            "[  35,   30/ 282]: 0.0723 | 0.0695 | 97.90% | 0.0686\n",
            "[  35,  130/ 282]: 0.0740 | 0.0756 | 97.79% | 0.0686\n",
            "[  35,  230/ 282]: 0.0753 | 0.0710 | 97.75% | 0.0686\n",
            "[  36,   48/ 282]: 0.0721 | 0.0707 | 97.73% | 0.0686\n",
            "[  36,  148/ 282]: 0.0741 | 0.0719 | 97.75% | 0.0686\n",
            "[  36,  248/ 282]: 0.0744 | 0.0705 | 97.74% | 0.0686\n",
            "[  37,   66/ 282]: 0.0734 | 0.0685 | 97.82% | 0.0686\n",
            "[  37,  166/ 282]: 0.0752 | 0.0742 | 97.68% | 0.0685\n",
            "[  37,  266/ 282]: 0.0754 | 0.0715 | 97.74% | 0.0685\n",
            "[  38,   84/ 282]: 0.0715 | 0.0701 | 97.82% | 0.0685\n",
            "[  38,  184/ 282]: 0.0735 | 0.0737 | 97.83% | 0.0685\n",
            "[  39,    2/ 282]: 0.0580 | 0.0703 | 97.80% | 0.0685\n",
            "[  39,  102/ 282]: 0.0742 | 0.0698 | 97.85% | 0.0685\n",
            "[  39,  202/ 282]: 0.0747 | 0.0732 | 97.85% | 0.0685\n",
            "[  40,   20/ 282]: 0.0630 | 0.0803 | 97.74% | 0.0685\n",
            "[  40,  120/ 282]: 0.0708 | 0.0729 | 97.85% | 0.0685\n",
            "[  40,  220/ 282]: 0.0729 | 0.0702 | 97.93% | 0.0685\n",
            "[  41,   38/ 282]: 0.0669 | 0.0695 | 97.81% | 0.0685\n",
            "[  41,  138/ 282]: 0.0717 | 0.0730 | 97.73% | 0.0685\n",
            "[  41,  238/ 282]: 0.0716 | 0.0733 | 97.71% | 0.0685\n",
            "[  42,   56/ 282]: 0.0708 | 0.0714 | 97.88% | 0.0685\n",
            "[  42,  156/ 282]: 0.0699 | 0.0710 | 97.78% | 0.0685\n",
            "[  42,  256/ 282]: 0.0707 | 0.0739 | 97.75% | 0.0685\n",
            "[  43,   74/ 282]: 0.0653 | 0.0717 | 97.91% | 0.0685\n",
            "[  43,  174/ 282]: 0.0677 | 0.0707 | 97.82% | 0.0685\n",
            "[  43,  274/ 282]: 0.0695 | 0.0723 | 97.91% | 0.0685\n",
            "[  44,   92/ 282]: 0.0704 | 0.0691 | 97.90% | 0.0685\n",
            "[  44,  192/ 282]: 0.0677 | 0.0677 | 98.00% | 0.0685\n",
            "[  45,   10/ 282]: 0.0873 | 0.0696 | 97.94% | 0.0677\n",
            "[  45,  110/ 282]: 0.0735 | 0.0691 | 97.84% | 0.0677\n",
            "[  45,  210/ 282]: 0.0696 | 0.0700 | 97.94% | 0.0677\n",
            "[  46,   28/ 282]: 0.0834 | 0.0700 | 97.79% | 0.0677\n",
            "[  46,  128/ 282]: 0.0756 | 0.0732 | 97.77% | 0.0677\n",
            "[  46,  228/ 282]: 0.0716 | 0.0725 | 97.83% | 0.0677\n",
            "[  47,   46/ 282]: 0.0709 | 0.0708 | 97.83% | 0.0677\n",
            "[  47,  146/ 282]: 0.0673 | 0.0698 | 97.87% | 0.0677\n",
            "[  47,  246/ 282]: 0.0666 | 0.0733 | 97.71% | 0.0677\n",
            "[  48,   64/ 282]: 0.0630 | 0.0748 | 97.78% | 0.0677\n",
            "[  48,  164/ 282]: 0.0678 | 0.0743 | 97.83% | 0.0677\n",
            "[  48,  264/ 282]: 0.0672 | 0.0690 | 97.97% | 0.0677\n",
            "[  49,   82/ 282]: 0.0623 | 0.0703 | 97.88% | 0.0677\n",
            "[  49,  182/ 282]: 0.0660 | 0.0700 | 97.85% | 0.0677\n",
            "[  49,  282/ 282]: 0.0671 | 0.0712 | 97.89% | 0.0677\n",
            "[  50,  100/ 282]: 0.0650 | 0.0729 | 97.79% | 0.0677\n",
            "[  50,  200/ 282]: 0.0653 | 0.0688 | 97.92% | 0.0677\n",
            "[  51,   18/ 282]: 0.0696 | 0.0708 | 97.87% | 0.0677\n",
            "[  51,  118/ 282]: 0.0659 | 0.0753 | 97.77% | 0.0677\n",
            "[  51,  218/ 282]: 0.0652 | 0.0707 | 97.89% | 0.0677\n",
            "[  52,   36/ 282]: 0.0639 | 0.0734 | 97.87% | 0.0677\n",
            "[  52,  136/ 282]: 0.0625 | 0.0706 | 97.98% | 0.0677\n",
            "[  52,  236/ 282]: 0.0644 | 0.0695 | 97.94% | 0.0677\n",
            "[  53,   54/ 282]: 0.0644 | 0.0690 | 97.93% | 0.0677\n",
            "[  53,  154/ 282]: 0.0647 | 0.0740 | 97.80% | 0.0677\n",
            "[  53,  254/ 282]: 0.0635 | 0.0679 | 97.98% | 0.0677\n",
            "[  54,   72/ 282]: 0.0585 | 0.0720 | 97.99% | 0.0677\n",
            "[  54,  172/ 282]: 0.0614 | 0.0759 | 97.91% | 0.0677\n",
            "[  54,  272/ 282]: 0.0618 | 0.0754 | 97.82% | 0.0677\n",
            "[  55,   90/ 282]: 0.0622 | 0.0730 | 97.90% | 0.0677\n",
            "[  55,  190/ 282]: 0.0613 | 0.0754 | 97.87% | 0.0677\n",
            "[  56,    8/ 282]: 0.0497 | 0.0701 | 98.00% | 0.0677\n",
            "[  56,  108/ 282]: 0.0594 | 0.0725 | 98.01% | 0.0677\n",
            "[  56,  208/ 282]: 0.0594 | 0.0743 | 97.98% | 0.0677\n",
            "[  57,   26/ 282]: 0.0616 | 0.0726 | 97.97% | 0.0677\n",
            "[  57,  126/ 282]: 0.0602 | 0.0776 | 97.92% | 0.0677\n",
            "[  57,  226/ 282]: 0.0606 | 0.0720 | 97.90% | 0.0677\n",
            "[  58,   44/ 282]: 0.0644 | 0.0710 | 98.01% | 0.0677\n",
            "[  58,  144/ 282]: 0.0597 | 0.0741 | 97.79% | 0.0677\n",
            "[  58,  244/ 282]: 0.0595 | 0.0731 | 97.99% | 0.0677\n",
            "[  59,   62/ 282]: 0.0633 | 0.0748 | 97.97% | 0.0677\n",
            "[  59,  162/ 282]: 0.0619 | 0.0765 | 97.89% | 0.0677\n",
            "[  59,  262/ 282]: 0.0613 | 0.0727 | 97.85% | 0.0677\n",
            "[  60,   80/ 282]: 0.0667 | 0.0700 | 97.91% | 0.0677\n",
            "[  60,  180/ 282]: 0.0620 | 0.0749 | 97.92% | 0.0677\n",
            "[  60,  280/ 282]: 0.0604 | 0.0735 | 97.90% | 0.0677\n",
            "[  61,   98/ 282]: 0.0671 | 0.0772 | 97.82% | 0.0677\n",
            "[  61,  198/ 282]: 0.0651 | 0.0744 | 97.98% | 0.0677\n",
            "[  62,   16/ 282]: 0.0635 | 0.0728 | 97.98% | 0.0677\n",
            "[  62,  116/ 282]: 0.0597 | 0.0794 | 97.84% | 0.0677\n",
            "[  62,  216/ 282]: 0.0608 | 0.0761 | 97.71% | 0.0677\n",
            "[  63,   34/ 282]: 0.0573 | 0.0770 | 97.81% | 0.0677\n",
            "[  63,  134/ 282]: 0.0587 | 0.0753 | 98.05% | 0.0677\n",
            "[  63,  234/ 282]: 0.0589 | 0.0758 | 97.95% | 0.0677\n",
            "[  64,   52/ 282]: 0.0575 | 0.0796 | 97.83% | 0.0677\n",
            "[  64,  152/ 282]: 0.0560 | 0.0714 | 97.87% | 0.0677\n",
            "[  64,  252/ 282]: 0.0559 | 0.0747 | 97.95% | 0.0677\n",
            "[  65,   70/ 282]: 0.0535 | 0.0731 | 97.97% | 0.0677\n",
            "[  65,  170/ 282]: 0.0596 | 0.0773 | 97.97% | 0.0677\n",
            "[  65,  270/ 282]: 0.0596 | 0.0742 | 97.90% | 0.0677\n",
            "[  66,   88/ 282]: 0.0569 | 0.0748 | 97.88% | 0.0677\n",
            "[  66,  188/ 282]: 0.0557 | 0.0732 | 97.95% | 0.0677\n",
            "[  67,    6/ 282]: 0.0673 | 0.0822 | 97.65% | 0.0677\n",
            "[  67,  106/ 282]: 0.0550 | 0.0753 | 97.91% | 0.0677\n",
            "[  67,  206/ 282]: 0.0546 | 0.0769 | 97.94% | 0.0677\n",
            "[  68,   24/ 282]: 0.0550 | 0.0726 | 98.04% | 0.0677\n",
            "[  68,  124/ 282]: 0.0560 | 0.0749 | 97.98% | 0.0677\n",
            "[  68,  224/ 282]: 0.0564 | 0.0767 | 97.85% | 0.0677\n",
            "[  69,   42/ 282]: 0.0607 | 0.0734 | 97.88% | 0.0677\n",
            "[  69,  142/ 282]: 0.0588 | 0.0735 | 97.94% | 0.0677\n",
            "[  69,  242/ 282]: 0.0571 | 0.0740 | 98.00% | 0.0677\n",
            "[  70,   60/ 282]: 0.0553 | 0.0766 | 97.87% | 0.0677\n",
            "[  70,  160/ 282]: 0.0549 | 0.0751 | 97.97% | 0.0677\n",
            "[  70,  260/ 282]: 0.0534 | 0.0752 | 97.97% | 0.0677\n",
            "[  71,   78/ 282]: 0.0510 | 0.0760 | 97.99% | 0.0677\n",
            "[  71,  178/ 282]: 0.0516 | 0.0762 | 97.98% | 0.0677\n",
            "[  71,  278/ 282]: 0.0517 | 0.0758 | 97.98% | 0.0677\n",
            "[  72,   96/ 282]: 0.0513 | 0.0757 | 97.95% | 0.0677\n",
            "[  72,  196/ 282]: 0.0516 | 0.0767 | 97.98% | 0.0677\n",
            "[  73,   14/ 282]: 0.0567 | 0.0762 | 97.99% | 0.0677\n",
            "[  73,  114/ 282]: 0.0514 | 0.0758 | 97.95% | 0.0677\n",
            "[  73,  214/ 282]: 0.0506 | 0.0760 | 97.94% | 0.0677\n",
            "[  74,   32/ 282]: 0.0534 | 0.0752 | 98.00% | 0.0677\n",
            "[  74,  132/ 282]: 0.0522 | 0.0759 | 97.98% | 0.0677\n",
            "[  74,  232/ 282]: 0.0519 | 0.0759 | 97.95% | 0.0677\n",
            "[  75,   50/ 282]: 0.0544 | 0.0765 | 97.99% | 0.0677\n",
            "[  75,  150/ 282]: 0.0512 | 0.0755 | 97.97% | 0.0677\n",
            "[  75,  250/ 282]: 0.0513 | 0.0750 | 97.98% | 0.0677\n",
            "[  76,   68/ 282]: 0.0514 | 0.0764 | 97.99% | 0.0677\n",
            "[  76,  168/ 282]: 0.0523 | 0.0759 | 97.92% | 0.0677\n",
            "[  76,  268/ 282]: 0.0525 | 0.0762 | 97.95% | 0.0677\n",
            "[  77,   86/ 282]: 0.0528 | 0.0773 | 97.99% | 0.0677\n",
            "[  77,  186/ 282]: 0.0520 | 0.0770 | 97.98% | 0.0677\n",
            "[  78,    4/ 282]: 0.0336 | 0.0759 | 97.98% | 0.0677\n",
            "[  78,  104/ 282]: 0.0537 | 0.0751 | 98.02% | 0.0677\n",
            "[  78,  204/ 282]: 0.0524 | 0.0764 | 97.97% | 0.0677\n",
            "[  79,   22/ 282]: 0.0574 | 0.0770 | 97.97% | 0.0677\n",
            "[  79,  122/ 282]: 0.0531 | 0.0765 | 97.99% | 0.0677\n",
            "[  79,  222/ 282]: 0.0538 | 0.0759 | 97.97% | 0.0677\n",
            "[  80,   40/ 282]: 0.0545 | 0.0772 | 97.98% | 0.0677\n",
            "[  80,  140/ 282]: 0.0489 | 0.0769 | 98.01% | 0.0677\n",
            "[  80,  240/ 282]: 0.0514 | 0.0760 | 97.95% | 0.0677\n",
            "[  81,   58/ 282]: 0.0510 | 0.0761 | 97.94% | 0.0677\n",
            "[  81,  158/ 282]: 0.0521 | 0.0763 | 97.97% | 0.0677\n",
            "[  81,  258/ 282]: 0.0512 | 0.0765 | 98.01% | 0.0677\n",
            "[  82,   76/ 282]: 0.0538 | 0.0764 | 98.02% | 0.0677\n",
            "[  82,  176/ 282]: 0.0519 | 0.0762 | 97.98% | 0.0677\n",
            "[  82,  276/ 282]: 0.0527 | 0.0770 | 97.94% | 0.0677\n",
            "[  83,   94/ 282]: 0.0498 | 0.0770 | 97.99% | 0.0677\n",
            "[  83,  194/ 282]: 0.0491 | 0.0768 | 97.98% | 0.0677\n",
            "[  84,   12/ 282]: 0.0524 | 0.0759 | 97.92% | 0.0677\n",
            "[  84,  112/ 282]: 0.0503 | 0.0768 | 97.94% | 0.0677\n",
            "[  84,  212/ 282]: 0.0507 | 0.0767 | 97.95% | 0.0677\n",
            "[  85,   30/ 282]: 0.0497 | 0.0776 | 97.97% | 0.0677\n",
            "[  85,  130/ 282]: 0.0515 | 0.0767 | 97.98% | 0.0677\n",
            "[  85,  230/ 282]: 0.0504 | 0.0772 | 98.00% | 0.0677\n",
            "[  86,   48/ 282]: 0.0521 | 0.0775 | 97.98% | 0.0677\n",
            "[  86,  148/ 282]: 0.0516 | 0.0772 | 97.98% | 0.0677\n",
            "[  86,  248/ 282]: 0.0527 | 0.0778 | 97.95% | 0.0677\n",
            "[  87,   66/ 282]: 0.0536 | 0.0770 | 97.98% | 0.0677\n",
            "[  87,  166/ 282]: 0.0518 | 0.0772 | 98.01% | 0.0677\n",
            "[  87,  266/ 282]: 0.0529 | 0.0766 | 98.01% | 0.0677\n",
            "[  88,   84/ 282]: 0.0515 | 0.0768 | 98.00% | 0.0677\n",
            "[  88,  184/ 282]: 0.0508 | 0.0769 | 97.99% | 0.0677\n",
            "[  89,    2/ 282]: 0.0545 | 0.0769 | 97.95% | 0.0677\n",
            "[  89,  102/ 282]: 0.0521 | 0.0776 | 98.00% | 0.0677\n",
            "[  89,  202/ 282]: 0.0507 | 0.0777 | 97.95% | 0.0677\n",
            "[  90,   20/ 282]: 0.0454 | 0.0773 | 97.99% | 0.0677\n",
            "[  90,  120/ 282]: 0.0498 | 0.0782 | 98.01% | 0.0677\n",
            "[  90,  220/ 282]: 0.0528 | 0.0779 | 97.95% | 0.0677\n",
            "[  91,   38/ 282]: 0.0534 | 0.0770 | 97.94% | 0.0677\n",
            "[  91,  138/ 282]: 0.0493 | 0.0775 | 97.94% | 0.0677\n",
            "[  91,  238/ 282]: 0.0500 | 0.0778 | 98.00% | 0.0677\n",
            "[  92,   56/ 282]: 0.0515 | 0.0779 | 97.98% | 0.0677\n",
            "[  92,  156/ 282]: 0.0514 | 0.0778 | 98.00% | 0.0677\n",
            "[  92,  256/ 282]: 0.0499 | 0.0774 | 97.98% | 0.0677\n",
            "[  93,   74/ 282]: 0.0512 | 0.0779 | 97.94% | 0.0677\n",
            "[  93,  174/ 282]: 0.0495 | 0.0777 | 98.04% | 0.0677\n",
            "[  93,  274/ 282]: 0.0492 | 0.0781 | 98.00% | 0.0677\n",
            "[  94,   92/ 282]: 0.0543 | 0.0780 | 97.98% | 0.0677\n",
            "[  94,  192/ 282]: 0.0507 | 0.0775 | 98.00% | 0.0677\n",
            "[  95,   10/ 282]: 0.0498 | 0.0779 | 97.95% | 0.0677\n",
            "[  95,  110/ 282]: 0.0495 | 0.0779 | 97.97% | 0.0677\n",
            "[  95,  210/ 282]: 0.0508 | 0.0777 | 97.92% | 0.0677\n",
            "[  96,   28/ 282]: 0.0447 | 0.0786 | 97.99% | 0.0677\n",
            "[  96,  128/ 282]: 0.0448 | 0.0787 | 97.98% | 0.0677\n",
            "[  96,  228/ 282]: 0.0467 | 0.0779 | 97.99% | 0.0677\n",
            "[  97,   46/ 282]: 0.0523 | 0.0781 | 97.93% | 0.0677\n",
            "[  97,  146/ 282]: 0.0514 | 0.0780 | 97.94% | 0.0677\n",
            "[  97,  246/ 282]: 0.0521 | 0.0787 | 97.98% | 0.0677\n",
            "[  98,   64/ 282]: 0.0523 | 0.0789 | 97.98% | 0.0677\n",
            "[  98,  164/ 282]: 0.0510 | 0.0784 | 98.00% | 0.0677\n",
            "[  98,  264/ 282]: 0.0495 | 0.0790 | 98.00% | 0.0677\n",
            "[  99,   82/ 282]: 0.0510 | 0.0778 | 97.97% | 0.0677\n",
            "[  99,  182/ 282]: 0.0490 | 0.0773 | 98.00% | 0.0677\n",
            "[  99,  282/ 282]: 0.0505 | 0.0791 | 97.94% | 0.0677\n",
            "Training of classifier [0,0] completed!\n",
            "========== Training classifier [1,0] ==========\n",
            "[epoch, batch/num_batches]: trainloss | validloss | best_validloss | accuracy\n",
            "[   0,  100/ 145]: 1.0875 | 0.2506 | 95.48% |   inf\n",
            "[   1,   55/ 145]: 0.2376 | 0.2242 | 96.63% | 0.2506\n",
            "[   2,   10/ 145]: 0.2485 | 0.2125 | 97.11% | 0.2242\n",
            "[   2,  110/ 145]: 0.2249 | 0.2049 | 97.39% | 0.2125\n",
            "[   3,   65/ 145]: 0.2182 | 0.1960 | 97.52% | 0.2049\n",
            "[   4,   20/ 145]: 0.2007 | 0.1913 | 97.69% | 0.1960\n",
            "[   4,  120/ 145]: 0.1957 | 0.1895 | 97.93% | 0.1913\n",
            "[   5,   75/ 145]: 0.1905 | 0.1820 | 97.80% | 0.1895\n",
            "[   6,   30/ 145]: 0.1778 | 0.1777 | 98.01% | 0.1820\n",
            "[   6,  130/ 145]: 0.1778 | 0.1737 | 98.10% | 0.1777\n",
            "[   7,   85/ 145]: 0.1804 | 0.1700 | 98.19% | 0.1737\n",
            "[   8,   40/ 145]: 0.1685 | 0.1699 | 98.27% | 0.1700\n",
            "[   8,  140/ 145]: 0.1662 | 0.1652 | 98.27% | 0.1699\n",
            "[   9,   95/ 145]: 0.1587 | 0.1613 | 98.31% | 0.1652\n",
            "[  10,   50/ 145]: 0.1567 | 0.1563 | 98.40% | 0.1613\n",
            "[  11,    5/ 145]: 0.1409 | 0.1521 | 98.44% | 0.1563\n",
            "[  11,  105/ 145]: 0.1499 | 0.1531 | 98.40% | 0.1521\n",
            "[  12,   60/ 145]: 0.1452 | 0.1485 | 98.42% | 0.1521\n",
            "[  13,   15/ 145]: 0.1374 | 0.1477 | 98.60% | 0.1485\n",
            "[  13,  115/ 145]: 0.1382 | 0.1431 | 98.60% | 0.1477\n",
            "[  14,   70/ 145]: 0.1390 | 0.1402 | 98.53% | 0.1431\n",
            "[  15,   25/ 145]: 0.1292 | 0.1406 | 98.60% | 0.1402\n",
            "[  15,  125/ 145]: 0.1314 | 0.1366 | 98.57% | 0.1402\n",
            "[  16,   80/ 145]: 0.1255 | 0.1391 | 98.57% | 0.1366\n",
            "[  17,   35/ 145]: 0.1185 | 0.1380 | 98.66% | 0.1366\n",
            "[  17,  135/ 145]: 0.1232 | 0.1337 | 98.64% | 0.1366\n",
            "[  18,   90/ 145]: 0.1234 | 0.1331 | 98.57% | 0.1337\n",
            "[  19,   45/ 145]: 0.1132 | 0.1298 | 98.38% | 0.1331\n",
            "[  19,  145/ 145]: 0.1175 | 0.1281 | 98.44% | 0.1298\n",
            "[  20,  100/ 145]: 0.1116 | 0.1318 | 98.53% | 0.1281\n",
            "[  21,   55/ 145]: 0.1124 | 0.1249 | 98.60% | 0.1281\n",
            "[  22,   10/ 145]: 0.1126 | 0.1266 | 98.77% | 0.1249\n",
            "[  22,  110/ 145]: 0.1080 | 0.1237 | 98.60% | 0.1249\n",
            "[  23,   65/ 145]: 0.1088 | 0.1248 | 98.68% | 0.1237\n",
            "[  24,   20/ 145]: 0.1077 | 0.1214 | 98.68% | 0.1237\n",
            "[  24,  120/ 145]: 0.1067 | 0.1197 | 98.68% | 0.1214\n",
            "[  25,   75/ 145]: 0.1070 | 0.1176 | 98.86% | 0.1197\n",
            "[  26,   30/ 145]: 0.0956 | 0.1214 | 98.75% | 0.1176\n",
            "[  26,  130/ 145]: 0.1004 | 0.1180 | 98.70% | 0.1176\n",
            "[  27,   85/ 145]: 0.1023 | 0.1156 | 98.75% | 0.1176\n",
            "[  28,   40/ 145]: 0.0953 | 0.1193 | 98.75% | 0.1156\n",
            "[  28,  140/ 145]: 0.0970 | 0.1167 | 98.79% | 0.1156\n",
            "[  29,   95/ 145]: 0.0950 | 0.1134 | 98.86% | 0.1156\n",
            "[  30,   50/ 145]: 0.0965 | 0.1110 | 98.66% | 0.1134\n",
            "[  31,    5/ 145]: 0.1056 | 0.1111 | 98.92% | 0.1110\n",
            "[  31,  105/ 145]: 0.0957 | 0.1138 | 98.77% | 0.1110\n",
            "[  32,   60/ 145]: 0.0955 | 0.1125 | 98.75% | 0.1110\n",
            "[  33,   15/ 145]: 0.0801 | 0.1112 | 98.79% | 0.1110\n",
            "[  33,  115/ 145]: 0.0879 | 0.1139 | 98.83% | 0.1110\n",
            "[  34,   70/ 145]: 0.0866 | 0.1096 | 98.86% | 0.1110\n",
            "[  35,   25/ 145]: 0.0830 | 0.1148 | 98.83% | 0.1096\n",
            "[  35,  125/ 145]: 0.0857 | 0.1103 | 98.88% | 0.1096\n",
            "[  36,   80/ 145]: 0.0854 | 0.1068 | 98.83% | 0.1096\n",
            "[  37,   35/ 145]: 0.0778 | 0.1106 | 98.81% | 0.1068\n",
            "[  37,  135/ 145]: 0.0839 | 0.1097 | 98.77% | 0.1068\n",
            "[  38,   90/ 145]: 0.0860 | 0.1046 | 98.66% | 0.1068\n",
            "[  39,   45/ 145]: 0.0837 | 0.1033 | 98.88% | 0.1046\n",
            "[  39,  145/ 145]: 0.0803 | 0.1060 | 98.79% | 0.1033\n",
            "[  40,  100/ 145]: 0.0833 | 0.1064 | 98.68% | 0.1033\n",
            "[  41,   55/ 145]: 0.0798 | 0.1022 | 98.68% | 0.1033\n",
            "[  42,   10/ 145]: 0.0773 | 0.1057 | 98.86% | 0.1022\n",
            "[  42,  110/ 145]: 0.0818 | 0.1017 | 98.83% | 0.1022\n",
            "[  43,   65/ 145]: 0.0681 | 0.1079 | 98.77% | 0.1017\n",
            "[  44,   20/ 145]: 0.0786 | 0.1022 | 98.86% | 0.1017\n",
            "[  44,  120/ 145]: 0.0737 | 0.1068 | 98.79% | 0.1017\n",
            "[  45,   75/ 145]: 0.0792 | 0.1076 | 98.66% | 0.1017\n",
            "[  46,   30/ 145]: 0.0691 | 0.1094 | 98.88% | 0.1017\n",
            "[  46,  130/ 145]: 0.0712 | 0.1063 | 98.79% | 0.1017\n",
            "[  47,   85/ 145]: 0.0761 | 0.1045 | 98.73% | 0.1017\n",
            "[  48,   40/ 145]: 0.0729 | 0.1072 | 98.83% | 0.1017\n",
            "[  48,  140/ 145]: 0.0718 | 0.1035 | 98.86% | 0.1017\n",
            "[  49,   95/ 145]: 0.0723 | 0.1019 | 98.86% | 0.1017\n",
            "[  50,   50/ 145]: 0.0722 | 0.1000 | 98.60% | 0.1017\n",
            "[  51,    5/ 145]: 0.0707 | 0.1030 | 98.75% | 0.1000\n",
            "[  51,  105/ 145]: 0.0693 | 0.1058 | 98.92% | 0.1000\n",
            "[  52,   60/ 145]: 0.0733 | 0.1009 | 98.55% | 0.1000\n",
            "[  53,   15/ 145]: 0.0690 | 0.1074 | 98.79% | 0.1000\n",
            "[  53,  115/ 145]: 0.0668 | 0.1030 | 98.86% | 0.1000\n",
            "[  54,   70/ 145]: 0.0636 | 0.1036 | 98.88% | 0.1000\n",
            "[  55,   25/ 145]: 0.0635 | 0.1002 | 98.88% | 0.1000\n",
            "[  55,  125/ 145]: 0.0660 | 0.0956 | 98.90% | 0.1000\n",
            "[  56,   80/ 145]: 0.0637 | 0.0953 | 98.86% | 0.0956\n",
            "[  57,   35/ 145]: 0.0591 | 0.1023 | 98.70% | 0.0953\n",
            "[  57,  135/ 145]: 0.0638 | 0.0919 | 98.79% | 0.0953\n",
            "[  58,   90/ 145]: 0.0637 | 0.0971 | 98.77% | 0.0919\n",
            "[  59,   45/ 145]: 0.0643 | 0.1009 | 98.83% | 0.0919\n",
            "[  59,  145/ 145]: 0.0631 | 0.1002 | 98.79% | 0.0919\n",
            "[  60,  100/ 145]: 0.0571 | 0.0976 | 98.81% | 0.0919\n",
            "[  61,   55/ 145]: 0.0591 | 0.0979 | 98.88% | 0.0919\n",
            "[  62,   10/ 145]: 0.0564 | 0.0985 | 98.88% | 0.0919\n",
            "[  62,  110/ 145]: 0.0604 | 0.0987 | 98.92% | 0.0919\n",
            "[  63,   65/ 145]: 0.0562 | 0.0914 | 98.92% | 0.0919\n",
            "[  64,   20/ 145]: 0.0548 | 0.0958 | 98.92% | 0.0914\n",
            "[  64,  120/ 145]: 0.0576 | 0.0961 | 98.83% | 0.0914\n",
            "[  65,   75/ 145]: 0.0577 | 0.1010 | 98.90% | 0.0914\n",
            "[  66,   30/ 145]: 0.0486 | 0.0985 | 98.92% | 0.0914\n",
            "[  66,  130/ 145]: 0.0622 | 0.0947 | 98.92% | 0.0914\n",
            "[  67,   85/ 145]: 0.0560 | 0.0978 | 98.94% | 0.0914\n",
            "[  68,   40/ 145]: 0.0590 | 0.0976 | 98.94% | 0.0914\n",
            "[  68,  140/ 145]: 0.0569 | 0.0942 | 98.81% | 0.0914\n",
            "[  69,   95/ 145]: 0.0598 | 0.0939 | 98.90% | 0.0914\n",
            "[  70,   50/ 145]: 0.0522 | 0.0958 | 98.90% | 0.0914\n",
            "[  71,    5/ 145]: 0.0561 | 0.0948 | 98.88% | 0.0914\n",
            "[  71,  105/ 145]: 0.0510 | 0.0948 | 98.88% | 0.0914\n",
            "[  72,   60/ 145]: 0.0532 | 0.0948 | 98.90% | 0.0914\n",
            "[  73,   15/ 145]: 0.0583 | 0.0952 | 98.88% | 0.0914\n",
            "[  73,  115/ 145]: 0.0525 | 0.0952 | 98.92% | 0.0914\n",
            "[  74,   70/ 145]: 0.0516 | 0.0952 | 98.88% | 0.0914\n",
            "[  75,   25/ 145]: 0.0533 | 0.0944 | 98.92% | 0.0914\n",
            "[  75,  125/ 145]: 0.0539 | 0.0955 | 98.88% | 0.0914\n",
            "[  76,   80/ 145]: 0.0534 | 0.0952 | 98.90% | 0.0914\n",
            "[  77,   35/ 145]: 0.0565 | 0.0957 | 98.88% | 0.0914\n",
            "[  77,  135/ 145]: 0.0537 | 0.0944 | 98.90% | 0.0914\n",
            "[  78,   90/ 145]: 0.0500 | 0.0949 | 98.88% | 0.0914\n",
            "[  79,   45/ 145]: 0.0487 | 0.0954 | 98.90% | 0.0914\n",
            "[  79,  145/ 145]: 0.0527 | 0.0945 | 98.88% | 0.0914\n",
            "[  80,  100/ 145]: 0.0503 | 0.0950 | 98.90% | 0.0914\n",
            "[  81,   55/ 145]: 0.0503 | 0.0945 | 98.88% | 0.0914\n",
            "[  82,   10/ 145]: 0.0558 | 0.0946 | 98.88% | 0.0914\n",
            "[  82,  110/ 145]: 0.0530 | 0.0938 | 98.88% | 0.0914\n",
            "[  83,   65/ 145]: 0.0534 | 0.0934 | 98.83% | 0.0914\n",
            "[  84,   20/ 145]: 0.0504 | 0.0956 | 98.86% | 0.0914\n",
            "[  84,  120/ 145]: 0.0544 | 0.0934 | 98.86% | 0.0914\n",
            "[  85,   75/ 145]: 0.0492 | 0.0939 | 98.83% | 0.0914\n",
            "[  86,   30/ 145]: 0.0517 | 0.0957 | 98.88% | 0.0914\n",
            "[  86,  130/ 145]: 0.0518 | 0.0929 | 98.86% | 0.0914\n",
            "[  87,   85/ 145]: 0.0499 | 0.0948 | 98.86% | 0.0914\n",
            "[  88,   40/ 145]: 0.0493 | 0.0955 | 98.88% | 0.0914\n",
            "[  88,  140/ 145]: 0.0502 | 0.0952 | 98.90% | 0.0914\n",
            "[  89,   95/ 145]: 0.0513 | 0.0954 | 98.88% | 0.0914\n",
            "[  90,   50/ 145]: 0.0536 | 0.0968 | 98.83% | 0.0914\n",
            "[  91,    5/ 145]: 0.0459 | 0.0951 | 98.88% | 0.0914\n",
            "[  91,  105/ 145]: 0.0512 | 0.0951 | 98.86% | 0.0914\n",
            "[  92,   60/ 145]: 0.0510 | 0.0966 | 98.88% | 0.0914\n",
            "[  93,   15/ 145]: 0.0458 | 0.0957 | 98.92% | 0.0914\n",
            "[  93,  115/ 145]: 0.0514 | 0.0951 | 98.90% | 0.0914\n",
            "[  94,   70/ 145]: 0.0526 | 0.0957 | 98.88% | 0.0914\n",
            "[  95,   25/ 145]: 0.0501 | 0.0956 | 98.90% | 0.0914\n",
            "[  95,  125/ 145]: 0.0517 | 0.0955 | 98.90% | 0.0914\n",
            "[  96,   80/ 145]: 0.0554 | 0.0950 | 98.90% | 0.0914\n",
            "[  97,   35/ 145]: 0.0502 | 0.0950 | 98.90% | 0.0914\n",
            "[  97,  135/ 145]: 0.0496 | 0.0956 | 98.88% | 0.0914\n",
            "[  98,   90/ 145]: 0.0494 | 0.0953 | 98.86% | 0.0914\n",
            "[  99,   45/ 145]: 0.0512 | 0.0953 | 98.90% | 0.0914\n",
            "[  99,  145/ 145]: 0.0509 | 0.0964 | 98.86% | 0.0914\n",
            "Training of classifier [1,0] completed!\n",
            "========== Training classifier [1,1] ==========\n",
            "[epoch, batch/num_batches]: trainloss | validloss | best_validloss | accuracy\n",
            "[   0,  100/ 137]: 0.9533 | 0.1853 | 93.77% |   inf\n",
            "[   1,   63/ 137]: 0.2031 | 0.1390 | 95.35% | 0.1853\n",
            "[   2,   26/ 137]: 0.1820 | 0.1419 | 95.46% | 0.1390\n",
            "[   2,  126/ 137]: 0.1789 | 0.1241 | 96.17% | 0.1390\n",
            "[   3,   89/ 137]: 0.1603 | 0.1178 | 96.29% | 0.1241\n",
            "[   4,   52/ 137]: 0.1513 | 0.1010 | 97.02% | 0.1178\n",
            "[   5,   15/ 137]: 0.1209 | 0.0942 | 97.07% | 0.1010\n",
            "[   5,  115/ 137]: 0.1210 | 0.0870 | 97.39% | 0.0942\n",
            "[   6,   78/ 137]: 0.1206 | 0.0870 | 97.34% | 0.0870\n",
            "[   7,   41/ 137]: 0.1055 | 0.0830 | 97.39% | 0.0870\n",
            "[   8,    4/ 137]: 0.1306 | 0.0861 | 97.16% | 0.0830\n",
            "[   8,  104/ 137]: 0.1036 | 0.0795 | 97.57% | 0.0830\n",
            "[   9,   67/ 137]: 0.0992 | 0.0775 | 97.55% | 0.0795\n",
            "[  10,   30/ 137]: 0.0926 | 0.0792 | 97.64% | 0.0775\n",
            "[  10,  130/ 137]: 0.0958 | 0.0765 | 97.62% | 0.0775\n",
            "[  11,   93/ 137]: 0.0871 | 0.0743 | 97.87% | 0.0765\n",
            "[  12,   56/ 137]: 0.0817 | 0.0759 | 97.48% | 0.0743\n",
            "[  13,   19/ 137]: 0.0817 | 0.0771 | 97.89% | 0.0743\n",
            "[  13,  119/ 137]: 0.0854 | 0.0758 | 97.76% | 0.0743\n",
            "[  14,   82/ 137]: 0.0803 | 0.0774 | 97.64% | 0.0743\n",
            "[  15,   45/ 137]: 0.0794 | 0.0789 | 97.71% | 0.0743\n",
            "[  16,    8/ 137]: 0.0885 | 0.0739 | 97.98% | 0.0743\n",
            "[  16,  108/ 137]: 0.0799 | 0.0757 | 97.87% | 0.0739\n",
            "[  17,   71/ 137]: 0.0791 | 0.0747 | 97.98% | 0.0739\n",
            "[  18,   34/ 137]: 0.0712 | 0.0805 | 97.82% | 0.0739\n",
            "[  18,  134/ 137]: 0.0769 | 0.0777 | 97.78% | 0.0739\n",
            "[  19,   97/ 137]: 0.0788 | 0.0784 | 97.76% | 0.0739\n",
            "[  20,   60/ 137]: 0.0741 | 0.0787 | 97.76% | 0.0739\n",
            "[  21,   23/ 137]: 0.0719 | 0.0803 | 97.80% | 0.0739\n",
            "[  21,  123/ 137]: 0.0711 | 0.0737 | 97.92% | 0.0739\n",
            "[  22,   86/ 137]: 0.0692 | 0.0792 | 97.71% | 0.0737\n",
            "[  23,   49/ 137]: 0.0703 | 0.0793 | 97.80% | 0.0737\n",
            "[  24,   12/ 137]: 0.0594 | 0.0758 | 98.05% | 0.0737\n",
            "[  24,  112/ 137]: 0.0691 | 0.0811 | 97.71% | 0.0737\n",
            "[  25,   75/ 137]: 0.0684 | 0.0817 | 97.89% | 0.0737\n",
            "[  26,   38/ 137]: 0.0693 | 0.0794 | 97.85% | 0.0737\n",
            "[  27,    1/ 137]: 0.1253 | 0.0840 | 97.78% | 0.0737\n",
            "[  27,  101/ 137]: 0.0637 | 0.0788 | 97.87% | 0.0737\n",
            "[  28,   64/ 137]: 0.0683 | 0.0769 | 97.89% | 0.0737\n",
            "[  29,   27/ 137]: 0.0667 | 0.0800 | 97.80% | 0.0737\n",
            "[  29,  127/ 137]: 0.0669 | 0.0779 | 97.89% | 0.0737\n",
            "[  30,   90/ 137]: 0.0628 | 0.0779 | 97.94% | 0.0737\n",
            "[  31,   53/ 137]: 0.0594 | 0.0822 | 97.94% | 0.0737\n",
            "[  32,   16/ 137]: 0.0693 | 0.0776 | 97.71% | 0.0737\n",
            "[  32,  116/ 137]: 0.0604 | 0.0756 | 97.92% | 0.0737\n",
            "[  33,   79/ 137]: 0.0602 | 0.0810 | 97.66% | 0.0737\n",
            "[  34,   42/ 137]: 0.0578 | 0.0773 | 97.76% | 0.0737\n",
            "[  35,    5/ 137]: 0.0738 | 0.0849 | 97.71% | 0.0737\n",
            "[  35,  105/ 137]: 0.0631 | 0.0806 | 97.89% | 0.0737\n",
            "[  36,   68/ 137]: 0.0572 | 0.0856 | 97.82% | 0.0737\n",
            "[  37,   31/ 137]: 0.0603 | 0.0817 | 97.89% | 0.0737\n",
            "[  37,  131/ 137]: 0.0564 | 0.0822 | 97.96% | 0.0737\n",
            "[  38,   94/ 137]: 0.0562 | 0.0843 | 97.82% | 0.0737\n",
            "[  39,   57/ 137]: 0.0619 | 0.0946 | 97.66% | 0.0737\n",
            "[  40,   20/ 137]: 0.0571 | 0.0862 | 97.92% | 0.0737\n",
            "[  40,  120/ 137]: 0.0536 | 0.0892 | 97.82% | 0.0737\n",
            "[  41,   83/ 137]: 0.0580 | 0.0883 | 98.01% | 0.0737\n",
            "[  42,   46/ 137]: 0.0563 | 0.0970 | 97.82% | 0.0737\n",
            "[  43,    9/ 137]: 0.0513 | 0.0995 | 97.80% | 0.0737\n",
            "[  43,  109/ 137]: 0.0546 | 0.0897 | 97.94% | 0.0737\n",
            "[  44,   72/ 137]: 0.0565 | 0.0915 | 97.98% | 0.0737\n",
            "[  45,   35/ 137]: 0.0443 | 0.0971 | 98.05% | 0.0737\n",
            "[  45,  135/ 137]: 0.0521 | 0.0949 | 98.01% | 0.0737\n",
            "[  46,   98/ 137]: 0.0507 | 0.0944 | 98.05% | 0.0737\n",
            "[  47,   61/ 137]: 0.0514 | 0.0937 | 98.05% | 0.0737\n",
            "[  48,   24/ 137]: 0.0449 | 0.0973 | 97.87% | 0.0737\n",
            "[  48,  124/ 137]: 0.0477 | 0.0974 | 97.98% | 0.0737\n",
            "[  49,   87/ 137]: 0.0519 | 0.0982 | 97.87% | 0.0737\n",
            "[  50,   50/ 137]: 0.0462 | 0.0967 | 98.12% | 0.0737\n",
            "[  51,   13/ 137]: 0.0575 | 0.0925 | 98.01% | 0.0737\n",
            "[  51,  113/ 137]: 0.0534 | 0.0976 | 97.98% | 0.0737\n",
            "[  52,   76/ 137]: 0.0491 | 0.0954 | 98.08% | 0.0737\n",
            "[  53,   39/ 137]: 0.0455 | 0.1009 | 98.12% | 0.0737\n",
            "[  54,    2/ 137]: 0.0416 | 0.1029 | 98.05% | 0.0737\n",
            "[  54,  102/ 137]: 0.0449 | 0.1012 | 98.05% | 0.0737\n",
            "[  55,   65/ 137]: 0.0456 | 0.1000 | 98.01% | 0.0737\n",
            "[  56,   28/ 137]: 0.0423 | 0.0994 | 98.08% | 0.0737\n",
            "[  56,  128/ 137]: 0.0473 | 0.1071 | 98.01% | 0.0737\n",
            "[  57,   91/ 137]: 0.0487 | 0.1015 | 97.87% | 0.0737\n",
            "[  58,   54/ 137]: 0.0410 | 0.1020 | 98.01% | 0.0737\n",
            "[  59,   17/ 137]: 0.0533 | 0.1083 | 97.87% | 0.0737\n",
            "[  59,  117/ 137]: 0.0475 | 0.1008 | 97.87% | 0.0737\n",
            "[  60,   80/ 137]: 0.0464 | 0.1054 | 98.08% | 0.0737\n",
            "[  61,   43/ 137]: 0.0523 | 0.1062 | 97.98% | 0.0737\n",
            "[  62,    6/ 137]: 0.0505 | 0.1053 | 97.98% | 0.0737\n",
            "[  62,  106/ 137]: 0.0439 | 0.1068 | 97.96% | 0.0737\n",
            "[  63,   69/ 137]: 0.0459 | 0.1029 | 97.98% | 0.0737\n",
            "[  64,   32/ 137]: 0.0452 | 0.1056 | 98.05% | 0.0737\n",
            "[  64,  132/ 137]: 0.0426 | 0.1073 | 97.89% | 0.0737\n",
            "[  65,   95/ 137]: 0.0416 | 0.1025 | 98.14% | 0.0737\n",
            "[  66,   58/ 137]: 0.0462 | 0.1006 | 97.98% | 0.0737\n",
            "[  67,   21/ 137]: 0.0409 | 0.1027 | 97.98% | 0.0737\n",
            "[  67,  121/ 137]: 0.0426 | 0.0994 | 98.14% | 0.0737\n",
            "[  68,   84/ 137]: 0.0411 | 0.1054 | 97.92% | 0.0737\n",
            "[  69,   47/ 137]: 0.0382 | 0.1092 | 98.01% | 0.0737\n",
            "[  70,   10/ 137]: 0.0452 | 0.1138 | 97.73% | 0.0737\n",
            "[  70,  110/ 137]: 0.0409 | 0.1069 | 97.96% | 0.0737\n",
            "[  71,   73/ 137]: 0.0397 | 0.1055 | 98.10% | 0.0737\n",
            "[  72,   36/ 137]: 0.0413 | 0.1067 | 98.03% | 0.0737\n",
            "[  72,  136/ 137]: 0.0401 | 0.1055 | 98.01% | 0.0737\n",
            "[  73,   99/ 137]: 0.0416 | 0.1059 | 98.05% | 0.0737\n",
            "[  74,   62/ 137]: 0.0426 | 0.1047 | 98.01% | 0.0737\n",
            "[  75,   25/ 137]: 0.0427 | 0.1047 | 98.03% | 0.0737\n",
            "[  75,  125/ 137]: 0.0400 | 0.1050 | 98.05% | 0.0737\n",
            "[  76,   88/ 137]: 0.0396 | 0.1044 | 98.05% | 0.0737\n",
            "[  77,   51/ 137]: 0.0388 | 0.1051 | 98.08% | 0.0737\n",
            "[  78,   14/ 137]: 0.0458 | 0.1051 | 98.03% | 0.0737\n",
            "[  78,  114/ 137]: 0.0423 | 0.1045 | 98.05% | 0.0737\n",
            "[  79,   77/ 137]: 0.0381 | 0.1061 | 98.03% | 0.0737\n",
            "[  80,   40/ 137]: 0.0412 | 0.1051 | 98.08% | 0.0737\n",
            "[  81,    3/ 137]: 0.0408 | 0.1049 | 98.08% | 0.0737\n",
            "[  81,  103/ 137]: 0.0412 | 0.1068 | 98.01% | 0.0737\n",
            "[  82,   66/ 137]: 0.0401 | 0.1062 | 97.96% | 0.0737\n",
            "[  83,   29/ 137]: 0.0402 | 0.1058 | 98.01% | 0.0737\n",
            "[  83,  129/ 137]: 0.0415 | 0.1050 | 97.98% | 0.0737\n",
            "[  84,   92/ 137]: 0.0376 | 0.1057 | 98.01% | 0.0737\n",
            "[  85,   55/ 137]: 0.0433 | 0.1041 | 97.98% | 0.0737\n",
            "[  86,   18/ 137]: 0.0376 | 0.1050 | 97.96% | 0.0737\n",
            "[  86,  118/ 137]: 0.0359 | 0.1064 | 97.96% | 0.0737\n",
            "[  87,   81/ 137]: 0.0348 | 0.1066 | 97.98% | 0.0737\n",
            "[  88,   44/ 137]: 0.0394 | 0.1053 | 98.03% | 0.0737\n",
            "[  89,    7/ 137]: 0.0383 | 0.1065 | 98.01% | 0.0737\n",
            "[  89,  107/ 137]: 0.0404 | 0.1057 | 98.03% | 0.0737\n",
            "[  90,   70/ 137]: 0.0425 | 0.1055 | 97.98% | 0.0737\n",
            "[  91,   33/ 137]: 0.0358 | 0.1060 | 97.98% | 0.0737\n",
            "[  91,  133/ 137]: 0.0390 | 0.1057 | 98.01% | 0.0737\n",
            "[  92,   96/ 137]: 0.0383 | 0.1052 | 97.94% | 0.0737\n",
            "[  93,   59/ 137]: 0.0406 | 0.1056 | 97.98% | 0.0737\n",
            "[  94,   22/ 137]: 0.0355 | 0.1062 | 97.98% | 0.0737\n",
            "[  94,  122/ 137]: 0.0381 | 0.1060 | 97.94% | 0.0737\n",
            "[  95,   85/ 137]: 0.0376 | 0.1061 | 97.96% | 0.0737\n",
            "[  96,   48/ 137]: 0.0334 | 0.1071 | 97.98% | 0.0737\n",
            "[  97,   11/ 137]: 0.0433 | 0.1059 | 98.01% | 0.0737\n",
            "[  97,  111/ 137]: 0.0399 | 0.1067 | 97.98% | 0.0737\n",
            "[  98,   74/ 137]: 0.0379 | 0.1073 | 98.01% | 0.0737\n",
            "[  99,   37/ 137]: 0.0507 | 0.1055 | 97.92% | 0.0737\n",
            "[  99,  137/ 137]: 0.0401 | 0.1062 | 97.98% | 0.0737\n",
            "Training of classifier [1,1] completed!\n",
            "========== Training classifier [2,0] ==========\n",
            "[epoch, batch/num_batches]: trainloss | validloss | best_validloss | accuracy\n",
            "[   1,   25/  75]: 0.0069 | 0.0086 | 99.92% |   inf\n",
            "[   2,   50/  75]: 0.0111 | 0.0050 | 99.87% | 0.0086\n",
            "[   3,   75/  75]: 0.0081 | 0.0037 | 99.83% | 0.0050\n",
            "[   5,   25/  75]: 0.0063 | 0.0037 | 99.83% | 0.0037\n",
            "[   6,   50/  75]: 0.0050 | 0.0039 | 99.83% | 0.0037\n",
            "[   7,   75/  75]: 0.0050 | 0.0036 | 99.83% | 0.0037\n",
            "[   9,   25/  75]: 0.0048 | 0.0038 | 99.83% | 0.0036\n",
            "[  10,   50/  75]: 0.0031 | 0.0025 | 99.92% | 0.0036\n",
            "[  11,   75/  75]: 0.0037 | 0.0020 | 99.92% | 0.0025\n",
            "[  13,   25/  75]: 0.0018 | 0.0024 | 99.92% | 0.0020\n",
            "[  14,   50/  75]: 0.0034 | 0.0037 | 99.87% | 0.0020\n",
            "[  15,   75/  75]: 0.0032 | 0.0034 | 99.87% | 0.0020\n",
            "[  17,   25/  75]: 0.0019 | 0.0035 | 99.87% | 0.0020\n",
            "[  18,   50/  75]: 0.0022 | 0.0033 | 99.92% | 0.0020\n",
            "[  19,   75/  75]: 0.0022 | 0.0032 | 99.92% | 0.0020\n",
            "[  21,   25/  75]: 0.0026 | 0.0036 | 99.92% | 0.0020\n",
            "[  22,   50/  75]: 0.0025 | 0.0035 | 99.92% | 0.0020\n",
            "[  23,   75/  75]: 0.0025 | 0.0031 | 99.92% | 0.0020\n",
            "[  25,   25/  75]: 0.0021 | 0.0033 | 99.92% | 0.0020\n",
            "[  26,   50/  75]: 0.0017 | 0.0033 | 99.92% | 0.0020\n",
            "[  27,   75/  75]: 0.0028 | 0.0033 | 99.92% | 0.0020\n",
            "[  29,   25/  75]: 0.0029 | 0.0037 | 99.92% | 0.0020\n",
            "[  30,   50/  75]: 0.0018 | 0.0036 | 99.92% | 0.0020\n",
            "[  31,   75/  75]: 0.0017 | 0.0034 | 99.92% | 0.0020\n",
            "[  33,   25/  75]: 0.0014 | 0.0035 | 99.92% | 0.0020\n",
            "[  34,   50/  75]: 0.0016 | 0.0039 | 99.92% | 0.0020\n",
            "[  35,   75/  75]: 0.0022 | 0.0038 | 99.92% | 0.0020\n",
            "[  37,   25/  75]: 0.0018 | 0.0040 | 99.92% | 0.0020\n",
            "[  38,   50/  75]: 0.0023 | 0.0040 | 99.92% | 0.0020\n",
            "[  39,   75/  75]: 0.0015 | 0.0046 | 99.92% | 0.0020\n",
            "[  41,   25/  75]: 0.0014 | 0.0045 | 99.92% | 0.0020\n",
            "[  42,   50/  75]: 0.0021 | 0.0036 | 99.92% | 0.0020\n",
            "[  43,   75/  75]: 0.0009 | 0.0043 | 99.92% | 0.0020\n",
            "[  45,   25/  75]: 0.0013 | 0.0044 | 99.92% | 0.0020\n",
            "[  46,   50/  75]: 0.0012 | 0.0045 | 99.92% | 0.0020\n",
            "[  47,   75/  75]: 0.0009 | 0.0048 | 99.92% | 0.0020\n",
            "[  49,   25/  75]: 0.0020 | 0.0049 | 99.92% | 0.0020\n",
            "[  50,   50/  75]: 0.0014 | 0.0050 | 99.92% | 0.0020\n",
            "[  51,   75/  75]: 0.0010 | 0.0049 | 99.92% | 0.0020\n",
            "[  53,   25/  75]: 0.0008 | 0.0051 | 99.92% | 0.0020\n",
            "[  54,   50/  75]: 0.0017 | 0.0045 | 99.92% | 0.0020\n",
            "[  55,   75/  75]: 0.0018 | 0.0041 | 99.92% | 0.0020\n",
            "[  57,   25/  75]: 0.0013 | 0.0042 | 99.92% | 0.0020\n",
            "[  58,   50/  75]: 0.0012 | 0.0047 | 99.92% | 0.0020\n",
            "[  59,   75/  75]: 0.0012 | 0.0048 | 99.92% | 0.0020\n",
            "[  61,   25/  75]: 0.0006 | 0.0046 | 99.92% | 0.0020\n",
            "[  62,   50/  75]: 0.0017 | 0.0044 | 99.92% | 0.0020\n",
            "[  63,   75/  75]: 0.0013 | 0.0043 | 99.92% | 0.0020\n",
            "[  65,   25/  75]: 0.0022 | 0.0046 | 99.92% | 0.0020\n",
            "[  66,   50/  75]: 0.0009 | 0.0048 | 99.92% | 0.0020\n",
            "[  67,   75/  75]: 0.0015 | 0.0044 | 99.92% | 0.0020\n",
            "[  69,   25/  75]: 0.0012 | 0.0047 | 99.92% | 0.0020\n",
            "[  70,   50/  75]: 0.0012 | 0.0048 | 99.92% | 0.0020\n",
            "[  71,   75/  75]: 0.0018 | 0.0047 | 99.92% | 0.0020\n",
            "[  73,   25/  75]: 0.0008 | 0.0047 | 99.92% | 0.0020\n",
            "[  74,   50/  75]: 0.0012 | 0.0046 | 99.92% | 0.0020\n",
            "[  75,   75/  75]: 0.0010 | 0.0046 | 99.92% | 0.0020\n",
            "[  77,   25/  75]: 0.0010 | 0.0046 | 99.92% | 0.0020\n",
            "[  78,   50/  75]: 0.0013 | 0.0046 | 99.92% | 0.0020\n",
            "[  79,   75/  75]: 0.0014 | 0.0046 | 99.92% | 0.0020\n",
            "[  81,   25/  75]: 0.0015 | 0.0047 | 99.92% | 0.0020\n",
            "[  82,   50/  75]: 0.0013 | 0.0046 | 99.92% | 0.0020\n",
            "[  83,   75/  75]: 0.0011 | 0.0046 | 99.92% | 0.0020\n",
            "[  85,   25/  75]: 0.0006 | 0.0047 | 99.92% | 0.0020\n",
            "[  86,   50/  75]: 0.0014 | 0.0046 | 99.92% | 0.0020\n",
            "[  87,   75/  75]: 0.0014 | 0.0046 | 99.92% | 0.0020\n",
            "[  89,   25/  75]: 0.0024 | 0.0046 | 99.92% | 0.0020\n",
            "[  90,   50/  75]: 0.0011 | 0.0046 | 99.92% | 0.0020\n",
            "[  91,   75/  75]: 0.0009 | 0.0045 | 99.92% | 0.0020\n",
            "[  93,   25/  75]: 0.0014 | 0.0045 | 99.92% | 0.0020\n",
            "[  94,   50/  75]: 0.0010 | 0.0045 | 99.92% | 0.0020\n",
            "[  95,   75/  75]: 0.0010 | 0.0045 | 99.92% | 0.0020\n",
            "[  97,   25/  75]: 0.0010 | 0.0045 | 99.92% | 0.0020\n",
            "[  98,   50/  75]: 0.0008 | 0.0044 | 99.92% | 0.0020\n",
            "[  99,   75/  75]: 0.0013 | 0.0044 | 99.92% | 0.0020\n",
            "Training of classifier [2,0] completed!\n",
            "========== Training classifier [2,1] ==========\n",
            "[epoch, batch/num_batches]: trainloss | validloss | best_validloss | accuracy\n",
            "[   1,   29/  71]: 0.1348 | 0.1297 | 95.88% |   inf\n",
            "[   2,   58/  71]: 0.1064 | 0.1099 | 96.47% | 0.1297\n",
            "[   4,   16/  71]: 0.0768 | 0.0972 | 97.32% | 0.1099\n",
            "[   5,   45/  71]: 0.0785 | 0.0912 | 97.23% | 0.0972\n",
            "[   7,    3/  71]: 0.0623 | 0.0787 | 97.45% | 0.0912\n",
            "[   8,   32/  71]: 0.0613 | 0.0758 | 97.63% | 0.0787\n",
            "[   9,   61/  71]: 0.0472 | 0.0940 | 97.67% | 0.0758\n",
            "[  11,   19/  71]: 0.0542 | 0.0774 | 97.72% | 0.0758\n",
            "[  12,   48/  71]: 0.0431 | 0.0775 | 97.99% | 0.0758\n",
            "[  14,    6/  71]: 0.0336 | 0.0828 | 97.94% | 0.0758\n",
            "[  15,   35/  71]: 0.0359 | 0.0824 | 98.21% | 0.0758\n",
            "[  16,   64/  71]: 0.0344 | 0.0819 | 98.17% | 0.0758\n",
            "[  18,   22/  71]: 0.0301 | 0.0727 | 98.21% | 0.0758\n",
            "[  19,   51/  71]: 0.0268 | 0.0795 | 98.34% | 0.0727\n",
            "[  21,    9/  71]: 0.0217 | 0.0897 | 97.94% | 0.0727\n",
            "[  22,   38/  71]: 0.0240 | 0.0845 | 98.17% | 0.0727\n",
            "[  23,   67/  71]: 0.0275 | 0.0795 | 98.43% | 0.0727\n",
            "[  25,   25/  71]: 0.0265 | 0.0853 | 98.34% | 0.0727\n",
            "[  26,   54/  71]: 0.0238 | 0.0783 | 98.34% | 0.0727\n",
            "[  28,   12/  71]: 0.0161 | 0.0858 | 98.17% | 0.0727\n",
            "[  29,   41/  71]: 0.0202 | 0.0907 | 98.21% | 0.0727\n",
            "[  30,   70/  71]: 0.0197 | 0.0867 | 98.12% | 0.0727\n",
            "[  32,   28/  71]: 0.0210 | 0.0856 | 98.26% | 0.0727\n",
            "[  33,   57/  71]: 0.0195 | 0.0759 | 98.21% | 0.0727\n",
            "[  35,   15/  71]: 0.0166 | 0.0891 | 98.03% | 0.0727\n",
            "[  36,   44/  71]: 0.0171 | 0.0861 | 98.17% | 0.0727\n",
            "[  38,    2/  71]: 0.0096 | 0.0865 | 98.21% | 0.0727\n",
            "[  39,   31/  71]: 0.0136 | 0.0898 | 98.17% | 0.0727\n",
            "[  40,   60/  71]: 0.0169 | 0.0816 | 98.12% | 0.0727\n",
            "[  42,   18/  71]: 0.0159 | 0.0896 | 98.30% | 0.0727\n",
            "[  43,   47/  71]: 0.0177 | 0.0901 | 98.30% | 0.0727\n",
            "[  45,    5/  71]: 0.0097 | 0.0896 | 98.30% | 0.0727\n",
            "[  46,   34/  71]: 0.0140 | 0.0832 | 98.34% | 0.0727\n",
            "[  47,   63/  71]: 0.0147 | 0.0833 | 98.21% | 0.0727\n",
            "[  49,   21/  71]: 0.0192 | 0.0781 | 98.34% | 0.0727\n",
            "[  50,   50/  71]: 0.0156 | 0.0824 | 98.21% | 0.0727\n",
            "[  52,    8/  71]: 0.0158 | 0.0806 | 98.26% | 0.0727\n",
            "[  53,   37/  71]: 0.0130 | 0.0905 | 98.26% | 0.0727\n",
            "[  54,   66/  71]: 0.0125 | 0.0931 | 98.26% | 0.0727\n",
            "[  56,   24/  71]: 0.0115 | 0.0978 | 98.26% | 0.0727\n",
            "[  57,   53/  71]: 0.0138 | 0.0927 | 98.39% | 0.0727\n",
            "[  59,   11/  71]: 0.0157 | 0.0892 | 98.21% | 0.0727\n",
            "[  60,   40/  71]: 0.0111 | 0.1009 | 98.30% | 0.0727\n",
            "[  61,   69/  71]: 0.0127 | 0.0904 | 98.26% | 0.0727\n",
            "[  63,   27/  71]: 0.0114 | 0.0895 | 98.39% | 0.0727\n",
            "[  64,   56/  71]: 0.0115 | 0.0909 | 98.34% | 0.0727\n",
            "[  66,   14/  71]: 0.0126 | 0.0906 | 98.30% | 0.0727\n",
            "[  67,   43/  71]: 0.0097 | 0.1051 | 98.21% | 0.0727\n",
            "[  69,    1/  71]: 0.0135 | 0.0971 | 98.26% | 0.0727\n",
            "[  70,   30/  71]: 0.0109 | 0.1032 | 98.21% | 0.0727\n",
            "[  71,   59/  71]: 0.0093 | 0.0994 | 98.30% | 0.0727\n",
            "[  73,   17/  71]: 0.0143 | 0.1008 | 98.30% | 0.0727\n",
            "[  74,   46/  71]: 0.0093 | 0.1008 | 98.30% | 0.0727\n",
            "[  76,    4/  71]: 0.0078 | 0.1005 | 98.34% | 0.0727\n",
            "[  77,   33/  71]: 0.0091 | 0.1005 | 98.30% | 0.0727\n",
            "[  78,   62/  71]: 0.0109 | 0.1008 | 98.39% | 0.0727\n",
            "[  80,   20/  71]: 0.0077 | 0.1009 | 98.34% | 0.0727\n",
            "[  81,   49/  71]: 0.0113 | 0.1046 | 98.34% | 0.0727\n",
            "[  83,    7/  71]: 0.0083 | 0.1022 | 98.39% | 0.0727\n",
            "[  84,   36/  71]: 0.0126 | 0.1032 | 98.34% | 0.0727\n",
            "[  85,   65/  71]: 0.0121 | 0.1013 | 98.39% | 0.0727\n",
            "[  87,   23/  71]: 0.0107 | 0.1029 | 98.34% | 0.0727\n",
            "[  88,   52/  71]: 0.0120 | 0.1014 | 98.34% | 0.0727\n",
            "[  90,   10/  71]: 0.0135 | 0.1024 | 98.34% | 0.0727\n",
            "[  91,   39/  71]: 0.0104 | 0.1022 | 98.34% | 0.0727\n",
            "[  92,   68/  71]: 0.0101 | 0.1023 | 98.34% | 0.0727\n",
            "[  94,   26/  71]: 0.0144 | 0.1033 | 98.34% | 0.0727\n",
            "[  95,   55/  71]: 0.0096 | 0.1049 | 98.34% | 0.0727\n",
            "[  97,   13/  71]: 0.0119 | 0.1044 | 98.34% | 0.0727\n",
            "[  98,   42/  71]: 0.0119 | 0.1024 | 98.34% | 0.0727\n",
            "[  99,   71/  71]: 0.0093 | 0.1043 | 98.34% | 0.0727\n",
            "Training of classifier [2,1] completed!\n",
            "========== Training classifier [2,2] ==========\n",
            "[epoch, batch/num_batches]: trainloss | validloss | best_validloss | accuracy\n",
            "[   1,   34/  66]: 0.0857 | 0.0801 | 97.94% |   inf\n",
            "[   3,    2/  66]: 0.0519 | 0.0662 | 98.45% | 0.0801\n",
            "[   4,   36/  66]: 0.0341 | 0.0465 | 98.97% | 0.0662\n",
            "[   6,    4/  66]: 0.0269 | 0.0418 | 98.87% | 0.0465\n",
            "[   7,   38/  66]: 0.0212 | 0.0410 | 99.06% | 0.0418\n",
            "[   9,    6/  66]: 0.0158 | 0.0365 | 99.20% | 0.0410\n",
            "[  10,   40/  66]: 0.0151 | 0.0370 | 99.16% | 0.0365\n",
            "[  12,    8/  66]: 0.0163 | 0.0358 | 99.30% | 0.0365\n",
            "[  13,   42/  66]: 0.0124 | 0.0389 | 99.20% | 0.0358\n",
            "[  15,   10/  66]: 0.0079 | 0.0376 | 99.34% | 0.0358\n",
            "[  16,   44/  66]: 0.0097 | 0.0374 | 99.30% | 0.0358\n",
            "[  18,   12/  66]: 0.0095 | 0.0372 | 99.39% | 0.0358\n",
            "[  19,   46/  66]: 0.0110 | 0.0387 | 99.34% | 0.0358\n",
            "[  21,   14/  66]: 0.0109 | 0.0387 | 99.20% | 0.0358\n",
            "[  22,   48/  66]: 0.0082 | 0.0403 | 99.30% | 0.0358\n",
            "[  24,   16/  66]: 0.0088 | 0.0388 | 99.34% | 0.0358\n",
            "[  25,   50/  66]: 0.0086 | 0.0393 | 99.30% | 0.0358\n",
            "[  27,   18/  66]: 0.0049 | 0.0402 | 99.30% | 0.0358\n",
            "[  28,   52/  66]: 0.0077 | 0.0376 | 99.39% | 0.0358\n",
            "[  30,   20/  66]: 0.0080 | 0.0417 | 99.34% | 0.0358\n",
            "[  31,   54/  66]: 0.0102 | 0.0376 | 99.44% | 0.0358\n",
            "[  33,   22/  66]: 0.0083 | 0.0380 | 99.39% | 0.0358\n",
            "[  34,   56/  66]: 0.0095 | 0.0384 | 99.39% | 0.0358\n",
            "[  36,   24/  66]: 0.0077 | 0.0391 | 99.48% | 0.0358\n",
            "[  37,   58/  66]: 0.0088 | 0.0404 | 99.48% | 0.0358\n",
            "[  39,   26/  66]: 0.0061 | 0.0421 | 99.39% | 0.0358\n",
            "[  40,   60/  66]: 0.0074 | 0.0377 | 99.48% | 0.0358\n",
            "[  42,   28/  66]: 0.0072 | 0.0375 | 99.44% | 0.0358\n",
            "[  43,   62/  66]: 0.0057 | 0.0409 | 99.34% | 0.0358\n",
            "[  45,   30/  66]: 0.0047 | 0.0404 | 99.39% | 0.0358\n",
            "[  46,   64/  66]: 0.0060 | 0.0409 | 99.39% | 0.0358\n",
            "[  48,   32/  66]: 0.0046 | 0.0398 | 99.44% | 0.0358\n",
            "[  49,   66/  66]: 0.0044 | 0.0435 | 99.34% | 0.0358\n",
            "[  51,   34/  66]: 0.0058 | 0.0424 | 99.39% | 0.0358\n",
            "[  53,    2/  66]: 0.0040 | 0.0419 | 99.44% | 0.0358\n",
            "[  54,   36/  66]: 0.0081 | 0.0412 | 99.48% | 0.0358\n",
            "[  56,    4/  66]: 0.0024 | 0.0428 | 99.39% | 0.0358\n",
            "[  57,   38/  66]: 0.0058 | 0.0391 | 99.39% | 0.0358\n",
            "[  59,    6/  66]: 0.0040 | 0.0404 | 99.48% | 0.0358\n",
            "[  60,   40/  66]: 0.0057 | 0.0440 | 99.44% | 0.0358\n",
            "[  62,    8/  66]: 0.0053 | 0.0401 | 99.48% | 0.0358\n",
            "[  63,   42/  66]: 0.0064 | 0.0405 | 99.48% | 0.0358\n",
            "[  65,   10/  66]: 0.0075 | 0.0417 | 99.48% | 0.0358\n",
            "[  66,   44/  66]: 0.0040 | 0.0427 | 99.48% | 0.0358\n",
            "[  68,   12/  66]: 0.0031 | 0.0449 | 99.48% | 0.0358\n",
            "[  69,   46/  66]: 0.0035 | 0.0421 | 99.48% | 0.0358\n",
            "[  71,   14/  66]: 0.0019 | 0.0431 | 99.48% | 0.0358\n",
            "[  72,   48/  66]: 0.0045 | 0.0431 | 99.48% | 0.0358\n",
            "[  74,   16/  66]: 0.0040 | 0.0432 | 99.48% | 0.0358\n",
            "[  75,   50/  66]: 0.0041 | 0.0433 | 99.48% | 0.0358\n",
            "[  77,   18/  66]: 0.0053 | 0.0433 | 99.48% | 0.0358\n",
            "[  78,   52/  66]: 0.0035 | 0.0428 | 99.48% | 0.0358\n",
            "[  80,   20/  66]: 0.0024 | 0.0431 | 99.48% | 0.0358\n",
            "[  81,   54/  66]: 0.0047 | 0.0430 | 99.48% | 0.0358\n",
            "[  83,   22/  66]: 0.0044 | 0.0431 | 99.48% | 0.0358\n",
            "[  84,   56/  66]: 0.0045 | 0.0434 | 99.48% | 0.0358\n",
            "[  86,   24/  66]: 0.0036 | 0.0437 | 99.48% | 0.0358\n",
            "[  87,   58/  66]: 0.0034 | 0.0433 | 99.48% | 0.0358\n",
            "[  89,   26/  66]: 0.0034 | 0.0434 | 99.48% | 0.0358\n",
            "[  90,   60/  66]: 0.0041 | 0.0436 | 99.48% | 0.0358\n",
            "[  92,   28/  66]: 0.0034 | 0.0436 | 99.48% | 0.0358\n",
            "[  93,   62/  66]: 0.0028 | 0.0439 | 99.48% | 0.0358\n",
            "[  95,   30/  66]: 0.0061 | 0.0439 | 99.48% | 0.0358\n",
            "[  96,   64/  66]: 0.0036 | 0.0444 | 99.48% | 0.0358\n",
            "[  98,   32/  66]: 0.0042 | 0.0445 | 99.48% | 0.0358\n",
            "[  99,   66/  66]: 0.0047 | 0.0444 | 99.48% | 0.0358\n",
            "Training of classifier [2,2] completed!\n",
            "========== Training classifier [2,3] ==========\n",
            "[epoch, batch/num_batches]: trainloss | validloss | best_validloss | accuracy\n",
            "[   1,   29/  71]: 0.0610 | 0.0241 | 99.24% |   inf\n",
            "[   2,   58/  71]: 0.0423 | 0.0147 | 99.42% | 0.0241\n",
            "[   4,   16/  71]: 0.0287 | 0.0126 | 99.51% | 0.0147\n",
            "[   5,   45/  71]: 0.0193 | 0.0114 | 99.51% | 0.0126\n",
            "[   7,    3/  71]: 0.0162 | 0.0111 | 99.60% | 0.0114\n",
            "[   8,   32/  71]: 0.0143 | 0.0110 | 99.60% | 0.0111\n",
            "[   9,   61/  71]: 0.0129 | 0.0093 | 99.78% | 0.0110\n",
            "[  11,   19/  71]: 0.0077 | 0.0094 | 99.73% | 0.0093\n",
            "[  12,   48/  71]: 0.0089 | 0.0089 | 99.82% | 0.0093\n",
            "[  14,    6/  71]: 0.0037 | 0.0084 | 99.78% | 0.0089\n",
            "[  15,   35/  71]: 0.0086 | 0.0083 | 99.78% | 0.0084\n",
            "[  16,   64/  71]: 0.0090 | 0.0081 | 99.82% | 0.0083\n",
            "[  18,   22/  71]: 0.0084 | 0.0085 | 99.78% | 0.0081\n",
            "[  19,   51/  71]: 0.0065 | 0.0083 | 99.78% | 0.0081\n",
            "[  21,    9/  71]: 0.0037 | 0.0088 | 99.78% | 0.0081\n",
            "[  22,   38/  71]: 0.0058 | 0.0085 | 99.78% | 0.0081\n",
            "[  23,   67/  71]: 0.0055 | 0.0087 | 99.78% | 0.0081\n",
            "[  25,   25/  71]: 0.0047 | 0.0086 | 99.78% | 0.0081\n",
            "[  26,   54/  71]: 0.0050 | 0.0087 | 99.78% | 0.0081\n",
            "[  28,   12/  71]: 0.0068 | 0.0083 | 99.78% | 0.0081\n",
            "[  29,   41/  71]: 0.0036 | 0.0088 | 99.78% | 0.0081\n",
            "[  30,   70/  71]: 0.0041 | 0.0089 | 99.78% | 0.0081\n",
            "[  32,   28/  71]: 0.0031 | 0.0084 | 99.78% | 0.0081\n",
            "[  33,   57/  71]: 0.0019 | 0.0091 | 99.78% | 0.0081\n",
            "[  35,   15/  71]: 0.0031 | 0.0085 | 99.78% | 0.0081\n",
            "[  36,   44/  71]: 0.0042 | 0.0094 | 99.78% | 0.0081\n",
            "[  38,    2/  71]: 0.0001 | 0.0095 | 99.78% | 0.0081\n",
            "[  39,   31/  71]: 0.0015 | 0.0093 | 99.78% | 0.0081\n",
            "[  40,   60/  71]: 0.0033 | 0.0097 | 99.78% | 0.0081\n",
            "[  42,   18/  71]: 0.0019 | 0.0087 | 99.78% | 0.0081\n",
            "[  43,   47/  71]: 0.0021 | 0.0090 | 99.78% | 0.0081\n",
            "[  45,    5/  71]: 0.0007 | 0.0096 | 99.78% | 0.0081\n",
            "[  46,   34/  71]: 0.0028 | 0.0088 | 99.78% | 0.0081\n",
            "[  47,   63/  71]: 0.0026 | 0.0089 | 99.78% | 0.0081\n",
            "[  49,   21/  71]: 0.0024 | 0.0098 | 99.78% | 0.0081\n",
            "[  50,   50/  71]: 0.0035 | 0.0093 | 99.78% | 0.0081\n",
            "[  52,    8/  71]: 0.0034 | 0.0096 | 99.78% | 0.0081\n",
            "[  53,   37/  71]: 0.0023 | 0.0094 | 99.78% | 0.0081\n",
            "[  54,   66/  71]: 0.0021 | 0.0089 | 99.78% | 0.0081\n",
            "[  56,   24/  71]: 0.0026 | 0.0093 | 99.78% | 0.0081\n",
            "[  57,   53/  71]: 0.0021 | 0.0096 | 99.78% | 0.0081\n",
            "[  59,   11/  71]: 0.0014 | 0.0094 | 99.78% | 0.0081\n",
            "[  60,   40/  71]: 0.0026 | 0.0094 | 99.78% | 0.0081\n",
            "[  61,   69/  71]: 0.0019 | 0.0095 | 99.78% | 0.0081\n",
            "[  63,   27/  71]: 0.0013 | 0.0097 | 99.78% | 0.0081\n",
            "[  64,   56/  71]: 0.0018 | 0.0094 | 99.78% | 0.0081\n",
            "[  66,   14/  71]: 0.0027 | 0.0091 | 99.78% | 0.0081\n",
            "[  67,   43/  71]: 0.0022 | 0.0096 | 99.78% | 0.0081\n",
            "[  69,    1/  71]: 0.0052 | 0.0095 | 99.78% | 0.0081\n",
            "[  70,   30/  71]: 0.0006 | 0.0096 | 99.78% | 0.0081\n",
            "[  71,   59/  71]: 0.0018 | 0.0095 | 99.78% | 0.0081\n",
            "[  73,   17/  71]: 0.0016 | 0.0095 | 99.78% | 0.0081\n",
            "[  74,   46/  71]: 0.0023 | 0.0096 | 99.78% | 0.0081\n",
            "[  76,    4/  71]: 0.0012 | 0.0095 | 99.78% | 0.0081\n",
            "[  77,   33/  71]: 0.0020 | 0.0095 | 99.78% | 0.0081\n",
            "[  78,   62/  71]: 0.0012 | 0.0097 | 99.78% | 0.0081\n",
            "[  80,   20/  71]: 0.0023 | 0.0097 | 99.78% | 0.0081\n",
            "[  81,   49/  71]: 0.0025 | 0.0098 | 99.78% | 0.0081\n",
            "[  83,    7/  71]: 0.0023 | 0.0098 | 99.78% | 0.0081\n",
            "[  84,   36/  71]: 0.0018 | 0.0098 | 99.78% | 0.0081\n",
            "[  85,   65/  71]: 0.0016 | 0.0098 | 99.78% | 0.0081\n",
            "[  87,   23/  71]: 0.0020 | 0.0098 | 99.78% | 0.0081\n",
            "[  88,   52/  71]: 0.0027 | 0.0098 | 99.78% | 0.0081\n",
            "[  90,   10/  71]: 0.0014 | 0.0098 | 99.78% | 0.0081\n",
            "[  91,   39/  71]: 0.0014 | 0.0099 | 99.78% | 0.0081\n",
            "[  92,   68/  71]: 0.0013 | 0.0098 | 99.78% | 0.0081\n",
            "[  94,   26/  71]: 0.0027 | 0.0098 | 99.78% | 0.0081\n",
            "[  95,   55/  71]: 0.0024 | 0.0099 | 99.78% | 0.0081\n",
            "[  97,   13/  71]: 0.0028 | 0.0099 | 99.78% | 0.0081\n",
            "[  98,   42/  71]: 0.0018 | 0.0098 | 99.78% | 0.0081\n",
            "[  99,   71/  71]: 0.0015 | 0.0099 | 99.78% | 0.0081\n",
            "Training of classifier [2,3] completed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DQmyZ7041Xlu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 574
        },
        "outputId": "7ec46059-636c-44c3-c465-d27f841b6087"
      },
      "source": [
        "fig = tree.plot_tree()\n",
        "fig.show()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQLmszqO7LZ-"
      },
      "source": [
        "## Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v0Rg_etMh28D"
      },
      "source": [
        "dataloader = torch.utils.data.DataLoader(SimpleDataset(testset), batch_size=256)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pvuT25U0heFh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ed93e84-b76d-4cab-8ca7-28c726799b93"
      },
      "source": [
        "right = 0\n",
        "total = 0\n",
        "\n",
        "for inputs, labels in dataloader:\n",
        "\n",
        "    for key in inputs:\n",
        "        inputs[key] = inputs[key].to(tree.device)\n",
        "\n",
        "    labels = labels.to(tree.device)\n",
        "\n",
        "    y_pred = tree.predict(inputs, return_probabilities=True)\n",
        "\n",
        "    right += (y_pred.argmax(axis=1)==labels).sum()\n",
        "    total += len(labels)\n",
        "\n",
        "print(\"ACCURACY: {:.2%}\".format(right/total))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ACCURACY: 93.85%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9qs3jYVOvdp3"
      },
      "source": [
        "# Classifier on its own"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1r4ySC5jFAdG"
      },
      "source": [
        "## Set training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KTR42s4Z4rKw"
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "classifier = SimpleClassifier(num_classes = 8).to('cuda')\n",
        "\n",
        "optimizer = torch.optim.SGD(classifier.parameters(), lr=2e-3)\n",
        "\n",
        "trainloader = DataLoader(SimpleDataset(trainset), batch_size=128, shuffle=True)\n",
        "validloader = DataLoader(SimpleDataset(validset), batch_size=256)\n",
        "\n",
        "loss_function = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, (100,150), 0.1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m5z2rKmpFC5P"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K1UmT0Jd5Ycs",
        "outputId": "3dbd4303-bb85-430f-e46e-fcabacf4f412"
      },
      "source": [
        "global_step = 0\n",
        "\n",
        "for epoch in range(200):\n",
        "\n",
        "    print(\"EPOCH [{}]\".format(epoch))\n",
        "\n",
        "    train_loss = 0.\n",
        "    epoch_step = 0\n",
        "\n",
        "    for samples, labels in trainloader:\n",
        "\n",
        "        for key in samples:\n",
        "            samples[key] = samples[key].to('cuda')\n",
        "\n",
        "        labels = labels.to('cuda')\n",
        "\n",
        "        y_pred = classifier(**samples)\n",
        "\n",
        "        loss = loss_function(y_pred, labels)\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        global_step += 1\n",
        "        epoch_step += 1\n",
        "\n",
        "        # Validation loop. Save progress and evaluate model performance.\n",
        "        if (global_step % 100 == 0) and (validloader is not None):\n",
        "\n",
        "            classifier.eval()\n",
        "\n",
        "            right = 0\n",
        "            total = 0\n",
        "            loss = 0.\n",
        "\n",
        "            with torch.no_grad():\n",
        "\n",
        "                for samples, labels in validloader:\n",
        "\n",
        "                    for key in samples:\n",
        "                        samples[key] = samples[key].to('cuda')\n",
        "\n",
        "                    labels = labels.to('cuda')\n",
        "\n",
        "                    y_pred = classifier(**samples)\n",
        "\n",
        "                    loss_ = loss_function(y_pred, labels)\n",
        "                    loss += loss_.item()\n",
        "\n",
        "                    total += len(labels)\n",
        "                    right += (y_pred.argmax(axis=-1) == labels).sum().item()\n",
        "\n",
        "                    valid_loss = loss / len(validloader)\n",
        "                    accuracy = right / total * 100\n",
        "            \n",
        "            classifier.train()\n",
        "            # print summary\n",
        "            print('Step: [{}/{}], train_loss: {:.4f}, valid_loss: {:.4f}, accuracy: {:.2f} %'\n",
        "                    .format(epoch_step,\n",
        "                            len(trainloader),\n",
        "                            train_loss / epoch_step, valid_loss, accuracy))\n",
        "    \n",
        "    scheduler.step()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH [0]\n",
            "Step: [100/282], train_loss: 2.3236, valid_loss: 1.7777, accuracy: 29.47 %\n",
            "Step: [200/282], train_loss: 2.0243, valid_loss: 1.5814, accuracy: 36.31 %\n",
            "EPOCH [1]\n",
            "Step: [18/282], train_loss: 1.3582, valid_loss: 1.2639, accuracy: 48.00 %\n",
            "Step: [118/282], train_loss: 1.3245, valid_loss: 1.1882, accuracy: 51.92 %\n",
            "Step: [218/282], train_loss: 1.3036, valid_loss: 1.1301, accuracy: 50.62 %\n",
            "EPOCH [2]\n",
            "Step: [36/282], train_loss: 1.2197, valid_loss: 1.1041, accuracy: 53.00 %\n",
            "Step: [136/282], train_loss: 1.2328, valid_loss: 1.0804, accuracy: 52.92 %\n",
            "Step: [236/282], train_loss: 1.2175, valid_loss: 1.0645, accuracy: 53.00 %\n",
            "EPOCH [3]\n",
            "Step: [54/282], train_loss: 1.1128, valid_loss: 1.0072, accuracy: 60.58 %\n",
            "Step: [154/282], train_loss: 1.0989, valid_loss: 0.9567, accuracy: 60.72 %\n",
            "Step: [254/282], train_loss: 1.0873, valid_loss: 0.9410, accuracy: 60.86 %\n",
            "EPOCH [4]\n",
            "Step: [72/282], train_loss: 1.1074, valid_loss: 0.9204, accuracy: 60.97 %\n",
            "Step: [172/282], train_loss: 1.0651, valid_loss: 0.9163, accuracy: 61.03 %\n",
            "Step: [272/282], train_loss: 1.0503, valid_loss: 0.8917, accuracy: 71.84 %\n",
            "EPOCH [5]\n",
            "Step: [90/282], train_loss: 0.9906, valid_loss: 0.8791, accuracy: 72.48 %\n",
            "Step: [190/282], train_loss: 0.9933, valid_loss: 0.8606, accuracy: 72.01 %\n",
            "EPOCH [6]\n",
            "Step: [8/282], train_loss: 1.0053, valid_loss: 0.8280, accuracy: 73.62 %\n",
            "Step: [108/282], train_loss: 0.9380, valid_loss: 0.7998, accuracy: 74.87 %\n",
            "Step: [208/282], train_loss: 0.9248, valid_loss: 0.8038, accuracy: 75.12 %\n",
            "EPOCH [7]\n",
            "Step: [26/282], train_loss: 0.8845, valid_loss: 0.7549, accuracy: 76.08 %\n",
            "Step: [126/282], train_loss: 0.8872, valid_loss: 0.7419, accuracy: 75.08 %\n",
            "Step: [226/282], train_loss: 0.8872, valid_loss: 0.7388, accuracy: 74.44 %\n",
            "EPOCH [8]\n",
            "Step: [44/282], train_loss: 0.8689, valid_loss: 0.7210, accuracy: 71.01 %\n",
            "Step: [144/282], train_loss: 0.8777, valid_loss: 0.7039, accuracy: 72.70 %\n",
            "Step: [244/282], train_loss: 0.8621, valid_loss: 0.5978, accuracy: 75.79 %\n",
            "EPOCH [9]\n",
            "Step: [62/282], train_loss: 0.7097, valid_loss: 0.5202, accuracy: 87.88 %\n",
            "Step: [162/282], train_loss: 0.6941, valid_loss: 0.4944, accuracy: 88.16 %\n",
            "Step: [262/282], train_loss: 0.6894, valid_loss: 0.4976, accuracy: 89.25 %\n",
            "EPOCH [10]\n",
            "Step: [80/282], train_loss: 0.6594, valid_loss: 0.4943, accuracy: 89.16 %\n",
            "Step: [180/282], train_loss: 0.6519, valid_loss: 0.4637, accuracy: 89.38 %\n",
            "Step: [280/282], train_loss: 0.6465, valid_loss: 0.4547, accuracy: 89.42 %\n",
            "EPOCH [11]\n",
            "Step: [98/282], train_loss: 0.6283, valid_loss: 0.4471, accuracy: 87.95 %\n",
            "Step: [198/282], train_loss: 0.6157, valid_loss: 0.4335, accuracy: 88.23 %\n",
            "EPOCH [12]\n",
            "Step: [16/282], train_loss: 0.6080, valid_loss: 0.4334, accuracy: 87.04 %\n",
            "Step: [116/282], train_loss: 0.5859, valid_loss: 0.4321, accuracy: 88.83 %\n",
            "Step: [216/282], train_loss: 0.5807, valid_loss: 0.4562, accuracy: 85.80 %\n",
            "EPOCH [13]\n",
            "Step: [34/282], train_loss: 0.5822, valid_loss: 0.3998, accuracy: 87.79 %\n",
            "Step: [134/282], train_loss: 0.5831, valid_loss: 0.3974, accuracy: 85.66 %\n",
            "Step: [234/282], train_loss: 0.5730, valid_loss: 0.3878, accuracy: 87.15 %\n",
            "EPOCH [14]\n",
            "Step: [52/282], train_loss: 0.5461, valid_loss: 0.3923, accuracy: 87.27 %\n",
            "Step: [152/282], train_loss: 0.5625, valid_loss: 0.3960, accuracy: 86.79 %\n",
            "Step: [252/282], train_loss: 0.5613, valid_loss: 0.3901, accuracy: 85.61 %\n",
            "EPOCH [15]\n",
            "Step: [70/282], train_loss: 0.5573, valid_loss: 0.3858, accuracy: 87.58 %\n",
            "Step: [170/282], train_loss: 0.5620, valid_loss: 0.3724, accuracy: 88.64 %\n",
            "Step: [270/282], train_loss: 0.5590, valid_loss: 0.3709, accuracy: 88.98 %\n",
            "EPOCH [16]\n",
            "Step: [88/282], train_loss: 0.5348, valid_loss: 0.3444, accuracy: 89.57 %\n",
            "Step: [188/282], train_loss: 0.5240, valid_loss: 0.3084, accuracy: 90.36 %\n",
            "EPOCH [17]\n",
            "Step: [6/282], train_loss: 0.7101, valid_loss: 0.3182, accuracy: 90.07 %\n",
            "Step: [106/282], train_loss: 0.4937, valid_loss: 0.2783, accuracy: 91.55 %\n",
            "Step: [206/282], train_loss: 0.4858, valid_loss: 0.2961, accuracy: 91.65 %\n",
            "EPOCH [18]\n",
            "Step: [24/282], train_loss: 0.4836, valid_loss: 0.2829, accuracy: 91.85 %\n",
            "Step: [124/282], train_loss: 0.4665, valid_loss: 0.3035, accuracy: 91.55 %\n",
            "Step: [224/282], train_loss: 0.4614, valid_loss: 0.2661, accuracy: 91.78 %\n",
            "EPOCH [19]\n",
            "Step: [42/282], train_loss: 0.4835, valid_loss: 0.2760, accuracy: 92.03 %\n",
            "Step: [142/282], train_loss: 0.4609, valid_loss: 0.2803, accuracy: 92.02 %\n",
            "Step: [242/282], train_loss: 0.4531, valid_loss: 0.2607, accuracy: 92.32 %\n",
            "EPOCH [20]\n",
            "Step: [60/282], train_loss: 0.4416, valid_loss: 0.2551, accuracy: 92.40 %\n",
            "Step: [160/282], train_loss: 0.4434, valid_loss: 0.2845, accuracy: 91.70 %\n",
            "Step: [260/282], train_loss: 0.4420, valid_loss: 0.2675, accuracy: 92.57 %\n",
            "EPOCH [21]\n",
            "Step: [78/282], train_loss: 0.4365, valid_loss: 0.2546, accuracy: 92.38 %\n",
            "Step: [178/282], train_loss: 0.4333, valid_loss: 0.2583, accuracy: 92.60 %\n",
            "Step: [278/282], train_loss: 0.4359, valid_loss: 0.2639, accuracy: 92.24 %\n",
            "EPOCH [22]\n",
            "Step: [96/282], train_loss: 0.4361, valid_loss: 0.2662, accuracy: 92.51 %\n",
            "Step: [196/282], train_loss: 0.4339, valid_loss: 0.2578, accuracy: 92.46 %\n",
            "EPOCH [23]\n",
            "Step: [14/282], train_loss: 0.3833, valid_loss: 0.2634, accuracy: 92.00 %\n",
            "Step: [114/282], train_loss: 0.4136, valid_loss: 0.2658, accuracy: 92.10 %\n",
            "Step: [214/282], train_loss: 0.4213, valid_loss: 0.2510, accuracy: 92.44 %\n",
            "EPOCH [24]\n",
            "Step: [32/282], train_loss: 0.4178, valid_loss: 0.2524, accuracy: 92.44 %\n",
            "Step: [132/282], train_loss: 0.4224, valid_loss: 0.2551, accuracy: 92.56 %\n",
            "Step: [232/282], train_loss: 0.4152, valid_loss: 0.2442, accuracy: 92.63 %\n",
            "EPOCH [25]\n",
            "Step: [50/282], train_loss: 0.4191, valid_loss: 0.2523, accuracy: 92.52 %\n",
            "Step: [150/282], train_loss: 0.4143, valid_loss: 0.2513, accuracy: 92.73 %\n",
            "Step: [250/282], train_loss: 0.4125, valid_loss: 0.2509, accuracy: 92.78 %\n",
            "EPOCH [26]\n",
            "Step: [68/282], train_loss: 0.4295, valid_loss: 0.2664, accuracy: 92.32 %\n",
            "Step: [168/282], train_loss: 0.4146, valid_loss: 0.2474, accuracy: 92.68 %\n",
            "Step: [268/282], train_loss: 0.4133, valid_loss: 0.2527, accuracy: 92.61 %\n",
            "EPOCH [27]\n",
            "Step: [86/282], train_loss: 0.3988, valid_loss: 0.2568, accuracy: 92.71 %\n",
            "Step: [186/282], train_loss: 0.4050, valid_loss: 0.2590, accuracy: 92.22 %\n",
            "EPOCH [28]\n",
            "Step: [4/282], train_loss: 0.4028, valid_loss: 0.2441, accuracy: 92.63 %\n",
            "Step: [104/282], train_loss: 0.4107, valid_loss: 0.2491, accuracy: 92.72 %\n",
            "Step: [204/282], train_loss: 0.4095, valid_loss: 0.2448, accuracy: 92.93 %\n",
            "EPOCH [29]\n",
            "Step: [22/282], train_loss: 0.4437, valid_loss: 0.2568, accuracy: 92.25 %\n",
            "Step: [122/282], train_loss: 0.4078, valid_loss: 0.2540, accuracy: 92.75 %\n",
            "Step: [222/282], train_loss: 0.4010, valid_loss: 0.2499, accuracy: 92.88 %\n",
            "EPOCH [30]\n",
            "Step: [40/282], train_loss: 0.3886, valid_loss: 0.2510, accuracy: 92.31 %\n",
            "Step: [140/282], train_loss: 0.4044, valid_loss: 0.2453, accuracy: 92.87 %\n",
            "Step: [240/282], train_loss: 0.4001, valid_loss: 0.2498, accuracy: 92.77 %\n",
            "EPOCH [31]\n",
            "Step: [58/282], train_loss: 0.3962, valid_loss: 0.2571, accuracy: 92.40 %\n",
            "Step: [158/282], train_loss: 0.4009, valid_loss: 0.2513, accuracy: 92.57 %\n",
            "Step: [258/282], train_loss: 0.4058, valid_loss: 0.2445, accuracy: 92.85 %\n",
            "EPOCH [32]\n",
            "Step: [76/282], train_loss: 0.3922, valid_loss: 0.2524, accuracy: 92.81 %\n",
            "Step: [176/282], train_loss: 0.3918, valid_loss: 0.2604, accuracy: 92.30 %\n",
            "Step: [276/282], train_loss: 0.3920, valid_loss: 0.2533, accuracy: 92.90 %\n",
            "EPOCH [33]\n",
            "Step: [94/282], train_loss: 0.3958, valid_loss: 0.2440, accuracy: 92.98 %\n",
            "Step: [194/282], train_loss: 0.3962, valid_loss: 0.2402, accuracy: 93.12 %\n",
            "EPOCH [34]\n",
            "Step: [12/282], train_loss: 0.3745, valid_loss: 0.2495, accuracy: 92.90 %\n",
            "Step: [112/282], train_loss: 0.3910, valid_loss: 0.2652, accuracy: 92.45 %\n",
            "Step: [212/282], train_loss: 0.3949, valid_loss: 0.2421, accuracy: 92.93 %\n",
            "EPOCH [35]\n",
            "Step: [30/282], train_loss: 0.4056, valid_loss: 0.2585, accuracy: 92.78 %\n",
            "Step: [130/282], train_loss: 0.3914, valid_loss: 0.2419, accuracy: 92.93 %\n",
            "Step: [230/282], train_loss: 0.3933, valid_loss: 0.2481, accuracy: 93.01 %\n",
            "EPOCH [36]\n",
            "Step: [48/282], train_loss: 0.4008, valid_loss: 0.2492, accuracy: 92.92 %\n",
            "Step: [148/282], train_loss: 0.3815, valid_loss: 0.2469, accuracy: 92.94 %\n",
            "Step: [248/282], train_loss: 0.3841, valid_loss: 0.2480, accuracy: 92.72 %\n",
            "EPOCH [37]\n",
            "Step: [66/282], train_loss: 0.3813, valid_loss: 0.2493, accuracy: 92.94 %\n",
            "Step: [166/282], train_loss: 0.3896, valid_loss: 0.2579, accuracy: 92.75 %\n",
            "Step: [266/282], train_loss: 0.3900, valid_loss: 0.2439, accuracy: 93.06 %\n",
            "EPOCH [38]\n",
            "Step: [84/282], train_loss: 0.3999, valid_loss: 0.2487, accuracy: 92.80 %\n",
            "Step: [184/282], train_loss: 0.3911, valid_loss: 0.2451, accuracy: 93.02 %\n",
            "EPOCH [39]\n",
            "Step: [2/282], train_loss: 0.6003, valid_loss: 0.2376, accuracy: 93.01 %\n",
            "Step: [102/282], train_loss: 0.3825, valid_loss: 0.2511, accuracy: 92.91 %\n",
            "Step: [202/282], train_loss: 0.3872, valid_loss: 0.2475, accuracy: 92.90 %\n",
            "EPOCH [40]\n",
            "Step: [20/282], train_loss: 0.4137, valid_loss: 0.2405, accuracy: 93.14 %\n",
            "Step: [120/282], train_loss: 0.4001, valid_loss: 0.2553, accuracy: 92.70 %\n",
            "Step: [220/282], train_loss: 0.3978, valid_loss: 0.2507, accuracy: 93.14 %\n",
            "EPOCH [41]\n",
            "Step: [38/282], train_loss: 0.3831, valid_loss: 0.2429, accuracy: 93.12 %\n",
            "Step: [138/282], train_loss: 0.3867, valid_loss: 0.2535, accuracy: 92.90 %\n",
            "Step: [238/282], train_loss: 0.3833, valid_loss: 0.2489, accuracy: 93.03 %\n",
            "EPOCH [42]\n",
            "Step: [56/282], train_loss: 0.3626, valid_loss: 0.2402, accuracy: 93.17 %\n",
            "Step: [156/282], train_loss: 0.3682, valid_loss: 0.2531, accuracy: 92.93 %\n",
            "Step: [256/282], train_loss: 0.3715, valid_loss: 0.2491, accuracy: 92.98 %\n",
            "EPOCH [43]\n",
            "Step: [74/282], train_loss: 0.4569, valid_loss: 0.2481, accuracy: 92.70 %\n",
            "Step: [174/282], train_loss: 0.4077, valid_loss: 0.2382, accuracy: 93.02 %\n",
            "Step: [274/282], train_loss: 0.3908, valid_loss: 0.2523, accuracy: 92.91 %\n",
            "EPOCH [44]\n",
            "Step: [92/282], train_loss: 0.3668, valid_loss: 0.2452, accuracy: 92.96 %\n",
            "Step: [192/282], train_loss: 0.3655, valid_loss: 0.2413, accuracy: 93.17 %\n",
            "EPOCH [45]\n",
            "Step: [10/282], train_loss: 0.3859, valid_loss: 0.2393, accuracy: 93.05 %\n",
            "Step: [110/282], train_loss: 0.3663, valid_loss: 0.2484, accuracy: 93.03 %\n",
            "Step: [210/282], train_loss: 0.3731, valid_loss: 0.2426, accuracy: 93.22 %\n",
            "EPOCH [46]\n",
            "Step: [28/282], train_loss: 0.3741, valid_loss: 0.2413, accuracy: 93.05 %\n",
            "Step: [128/282], train_loss: 0.3639, valid_loss: 0.2513, accuracy: 92.84 %\n",
            "Step: [228/282], train_loss: 0.3705, valid_loss: 0.2618, accuracy: 92.45 %\n",
            "EPOCH [47]\n",
            "Step: [46/282], train_loss: 0.3772, valid_loss: 0.2423, accuracy: 93.10 %\n",
            "Step: [146/282], train_loss: 0.3721, valid_loss: 0.2507, accuracy: 92.92 %\n",
            "Step: [246/282], train_loss: 0.3634, valid_loss: 0.2393, accuracy: 93.20 %\n",
            "EPOCH [48]\n",
            "Step: [64/282], train_loss: 0.3711, valid_loss: 0.2441, accuracy: 93.14 %\n",
            "Step: [164/282], train_loss: 0.3645, valid_loss: 0.2476, accuracy: 92.73 %\n",
            "Step: [264/282], train_loss: 0.3615, valid_loss: 0.2360, accuracy: 93.29 %\n",
            "EPOCH [49]\n",
            "Step: [82/282], train_loss: 0.3728, valid_loss: 0.2383, accuracy: 93.29 %\n",
            "Step: [182/282], train_loss: 0.3656, valid_loss: 0.2445, accuracy: 92.84 %\n",
            "Step: [282/282], train_loss: 0.3707, valid_loss: 0.3402, accuracy: 90.08 %\n",
            "EPOCH [50]\n",
            "Step: [100/282], train_loss: 0.3618, valid_loss: 0.2529, accuracy: 92.71 %\n",
            "Step: [200/282], train_loss: 0.3618, valid_loss: 0.2359, accuracy: 93.27 %\n",
            "EPOCH [51]\n",
            "Step: [18/282], train_loss: 0.3759, valid_loss: 0.2372, accuracy: 93.09 %\n",
            "Step: [118/282], train_loss: 0.3626, valid_loss: 0.2454, accuracy: 93.06 %\n",
            "Step: [218/282], train_loss: 0.3666, valid_loss: 0.2402, accuracy: 93.11 %\n",
            "EPOCH [52]\n",
            "Step: [36/282], train_loss: 0.3438, valid_loss: 0.2433, accuracy: 93.16 %\n",
            "Step: [136/282], train_loss: 0.3529, valid_loss: 0.2479, accuracy: 92.92 %\n",
            "Step: [236/282], train_loss: 0.3574, valid_loss: 0.2436, accuracy: 93.20 %\n",
            "EPOCH [53]\n",
            "Step: [54/282], train_loss: 0.3903, valid_loss: 0.2375, accuracy: 93.01 %\n",
            "Step: [154/282], train_loss: 0.3739, valid_loss: 0.2341, accuracy: 93.40 %\n",
            "Step: [254/282], train_loss: 0.3643, valid_loss: 0.2420, accuracy: 93.11 %\n",
            "EPOCH [54]\n",
            "Step: [72/282], train_loss: 0.3550, valid_loss: 0.2486, accuracy: 93.14 %\n",
            "Step: [172/282], train_loss: 0.3590, valid_loss: 0.2498, accuracy: 93.21 %\n",
            "Step: [272/282], train_loss: 0.3618, valid_loss: 0.2323, accuracy: 93.20 %\n",
            "EPOCH [55]\n",
            "Step: [90/282], train_loss: 0.3644, valid_loss: 0.2436, accuracy: 93.14 %\n",
            "Step: [190/282], train_loss: 0.3601, valid_loss: 0.2359, accuracy: 93.20 %\n",
            "EPOCH [56]\n",
            "Step: [8/282], train_loss: 0.3680, valid_loss: 0.2503, accuracy: 92.88 %\n",
            "Step: [108/282], train_loss: 0.3656, valid_loss: 0.2430, accuracy: 93.02 %\n",
            "Step: [208/282], train_loss: 0.3603, valid_loss: 0.2466, accuracy: 93.05 %\n",
            "EPOCH [57]\n",
            "Step: [26/282], train_loss: 0.3848, valid_loss: 0.2386, accuracy: 93.41 %\n",
            "Step: [126/282], train_loss: 0.3670, valid_loss: 0.2311, accuracy: 93.35 %\n",
            "Step: [226/282], train_loss: 0.3610, valid_loss: 0.2364, accuracy: 93.23 %\n",
            "EPOCH [58]\n",
            "Step: [44/282], train_loss: 0.3915, valid_loss: 0.2426, accuracy: 93.16 %\n",
            "Step: [144/282], train_loss: 0.3632, valid_loss: 0.2344, accuracy: 93.35 %\n",
            "Step: [244/282], train_loss: 0.3591, valid_loss: 0.2496, accuracy: 92.97 %\n",
            "EPOCH [59]\n",
            "Step: [62/282], train_loss: 0.3488, valid_loss: 0.2434, accuracy: 92.92 %\n",
            "Step: [162/282], train_loss: 0.3526, valid_loss: 0.2371, accuracy: 93.02 %\n",
            "Step: [262/282], train_loss: 0.3528, valid_loss: 0.2423, accuracy: 93.13 %\n",
            "EPOCH [60]\n",
            "Step: [80/282], train_loss: 0.3592, valid_loss: 0.2327, accuracy: 93.40 %\n",
            "Step: [180/282], train_loss: 0.3572, valid_loss: 0.2382, accuracy: 93.12 %\n",
            "Step: [280/282], train_loss: 0.3573, valid_loss: 0.2346, accuracy: 93.31 %\n",
            "EPOCH [61]\n",
            "Step: [98/282], train_loss: 0.3442, valid_loss: 0.2370, accuracy: 93.46 %\n",
            "Step: [198/282], train_loss: 0.3536, valid_loss: 0.2458, accuracy: 92.92 %\n",
            "EPOCH [62]\n",
            "Step: [16/282], train_loss: 0.3912, valid_loss: 0.2436, accuracy: 93.02 %\n",
            "Step: [116/282], train_loss: 0.3616, valid_loss: 0.2488, accuracy: 92.88 %\n",
            "Step: [216/282], train_loss: 0.3584, valid_loss: 0.2391, accuracy: 93.33 %\n",
            "EPOCH [63]\n",
            "Step: [34/282], train_loss: 0.3772, valid_loss: 0.2427, accuracy: 93.01 %\n",
            "Step: [134/282], train_loss: 0.3568, valid_loss: 0.2505, accuracy: 92.83 %\n",
            "Step: [234/282], train_loss: 0.3551, valid_loss: 0.2462, accuracy: 93.00 %\n",
            "EPOCH [64]\n",
            "Step: [52/282], train_loss: 0.3415, valid_loss: 0.2377, accuracy: 93.47 %\n",
            "Step: [152/282], train_loss: 0.3558, valid_loss: 0.2377, accuracy: 93.20 %\n",
            "Step: [252/282], train_loss: 0.3534, valid_loss: 0.2459, accuracy: 92.90 %\n",
            "EPOCH [65]\n",
            "Step: [70/282], train_loss: 0.3509, valid_loss: 0.2365, accuracy: 93.33 %\n",
            "Step: [170/282], train_loss: 0.3450, valid_loss: 0.2334, accuracy: 93.15 %\n",
            "Step: [270/282], train_loss: 0.3445, valid_loss: 0.2428, accuracy: 93.04 %\n",
            "EPOCH [66]\n",
            "Step: [88/282], train_loss: 0.3571, valid_loss: 0.2480, accuracy: 93.01 %\n",
            "Step: [188/282], train_loss: 0.3527, valid_loss: 0.2377, accuracy: 93.24 %\n",
            "EPOCH [67]\n",
            "Step: [6/282], train_loss: 0.3955, valid_loss: 0.2479, accuracy: 93.27 %\n",
            "Step: [106/282], train_loss: 0.3476, valid_loss: 0.2329, accuracy: 93.33 %\n",
            "Step: [206/282], train_loss: 0.3489, valid_loss: 0.2365, accuracy: 92.98 %\n",
            "EPOCH [68]\n",
            "Step: [24/282], train_loss: 0.3437, valid_loss: 0.2395, accuracy: 92.87 %\n",
            "Step: [124/282], train_loss: 0.3514, valid_loss: 0.2462, accuracy: 93.29 %\n",
            "Step: [224/282], train_loss: 0.3521, valid_loss: 0.2398, accuracy: 93.26 %\n",
            "EPOCH [69]\n",
            "Step: [42/282], train_loss: 0.3778, valid_loss: 0.2366, accuracy: 93.15 %\n",
            "Step: [142/282], train_loss: 0.3559, valid_loss: 0.2390, accuracy: 93.32 %\n",
            "Step: [242/282], train_loss: 0.3532, valid_loss: 0.2372, accuracy: 93.34 %\n",
            "EPOCH [70]\n",
            "Step: [60/282], train_loss: 0.3296, valid_loss: 0.2412, accuracy: 93.11 %\n",
            "Step: [160/282], train_loss: 0.3438, valid_loss: 0.2447, accuracy: 93.19 %\n",
            "Step: [260/282], train_loss: 0.3461, valid_loss: 0.2362, accuracy: 93.35 %\n",
            "EPOCH [71]\n",
            "Step: [78/282], train_loss: 0.3417, valid_loss: 0.2361, accuracy: 93.06 %\n",
            "Step: [178/282], train_loss: 0.3483, valid_loss: 0.2491, accuracy: 93.05 %\n",
            "Step: [278/282], train_loss: 0.3531, valid_loss: 0.2340, accuracy: 93.44 %\n",
            "EPOCH [72]\n",
            "Step: [96/282], train_loss: 0.3640, valid_loss: 0.2458, accuracy: 93.00 %\n",
            "Step: [196/282], train_loss: 0.3577, valid_loss: 0.2456, accuracy: 93.40 %\n",
            "EPOCH [73]\n",
            "Step: [14/282], train_loss: 0.3948, valid_loss: 0.2431, accuracy: 93.15 %\n",
            "Step: [114/282], train_loss: 0.3587, valid_loss: 0.2482, accuracy: 93.29 %\n",
            "Step: [214/282], train_loss: 0.3547, valid_loss: 0.2632, accuracy: 92.52 %\n",
            "EPOCH [74]\n",
            "Step: [32/282], train_loss: 0.3356, valid_loss: 0.2524, accuracy: 92.98 %\n",
            "Step: [132/282], train_loss: 0.3501, valid_loss: 0.2400, accuracy: 93.30 %\n",
            "Step: [232/282], train_loss: 0.3524, valid_loss: 0.2407, accuracy: 93.12 %\n",
            "EPOCH [75]\n",
            "Step: [50/282], train_loss: 0.3508, valid_loss: 0.2351, accuracy: 93.15 %\n",
            "Step: [150/282], train_loss: 0.3510, valid_loss: 0.2370, accuracy: 93.11 %\n",
            "Step: [250/282], train_loss: 0.3497, valid_loss: 0.2347, accuracy: 93.32 %\n",
            "EPOCH [76]\n",
            "Step: [68/282], train_loss: 0.3430, valid_loss: 0.2459, accuracy: 93.10 %\n",
            "Step: [168/282], train_loss: 0.3444, valid_loss: 0.2391, accuracy: 93.11 %\n",
            "Step: [268/282], train_loss: 0.3488, valid_loss: 0.2370, accuracy: 92.95 %\n",
            "EPOCH [77]\n",
            "Step: [86/282], train_loss: 0.3602, valid_loss: 0.2439, accuracy: 93.33 %\n",
            "Step: [186/282], train_loss: 0.3502, valid_loss: 0.2403, accuracy: 93.41 %\n",
            "EPOCH [78]\n",
            "Step: [4/282], train_loss: 0.3853, valid_loss: 0.2322, accuracy: 93.34 %\n",
            "Step: [104/282], train_loss: 0.3520, valid_loss: 0.2418, accuracy: 93.25 %\n",
            "Step: [204/282], train_loss: 0.3484, valid_loss: 0.2438, accuracy: 93.04 %\n",
            "EPOCH [79]\n",
            "Step: [22/282], train_loss: 0.3502, valid_loss: 0.2441, accuracy: 93.17 %\n",
            "Step: [122/282], train_loss: 0.3433, valid_loss: 0.2413, accuracy: 93.16 %\n",
            "Step: [222/282], train_loss: 0.3492, valid_loss: 0.2542, accuracy: 92.36 %\n",
            "EPOCH [80]\n",
            "Step: [40/282], train_loss: 0.3513, valid_loss: 0.2522, accuracy: 92.86 %\n",
            "Step: [140/282], train_loss: 0.3452, valid_loss: 0.2356, accuracy: 93.41 %\n",
            "Step: [240/282], train_loss: 0.3479, valid_loss: 0.2505, accuracy: 92.63 %\n",
            "EPOCH [81]\n",
            "Step: [58/282], train_loss: 0.3430, valid_loss: 0.2375, accuracy: 93.14 %\n",
            "Step: [158/282], train_loss: 0.3501, valid_loss: 0.2415, accuracy: 93.10 %\n",
            "Step: [258/282], train_loss: 0.3458, valid_loss: 0.2523, accuracy: 93.32 %\n",
            "EPOCH [82]\n",
            "Step: [76/282], train_loss: 0.3366, valid_loss: 0.2359, accuracy: 92.95 %\n",
            "Step: [176/282], train_loss: 0.3369, valid_loss: 0.2374, accuracy: 93.21 %\n",
            "Step: [276/282], train_loss: 0.3403, valid_loss: 0.2598, accuracy: 93.00 %\n",
            "EPOCH [83]\n",
            "Step: [94/282], train_loss: 0.3319, valid_loss: 0.2285, accuracy: 93.29 %\n",
            "Step: [194/282], train_loss: 0.3411, valid_loss: 0.2501, accuracy: 93.02 %\n",
            "EPOCH [84]\n",
            "Step: [12/282], train_loss: 0.3837, valid_loss: 0.2586, accuracy: 93.10 %\n",
            "Step: [112/282], train_loss: 0.3518, valid_loss: 0.2410, accuracy: 93.06 %\n",
            "Step: [212/282], train_loss: 0.3511, valid_loss: 0.2401, accuracy: 93.36 %\n",
            "EPOCH [85]\n",
            "Step: [30/282], train_loss: 0.3268, valid_loss: 0.2359, accuracy: 93.37 %\n",
            "Step: [130/282], train_loss: 0.3495, valid_loss: 0.2411, accuracy: 92.91 %\n",
            "Step: [230/282], train_loss: 0.3482, valid_loss: 0.2454, accuracy: 93.24 %\n",
            "EPOCH [86]\n",
            "Step: [48/282], train_loss: 0.3563, valid_loss: 0.2396, accuracy: 93.03 %\n",
            "Step: [148/282], train_loss: 0.3464, valid_loss: 0.2376, accuracy: 93.10 %\n",
            "Step: [248/282], train_loss: 0.3482, valid_loss: 0.2337, accuracy: 93.22 %\n",
            "EPOCH [87]\n",
            "Step: [66/282], train_loss: 0.3620, valid_loss: 0.2452, accuracy: 92.94 %\n",
            "Step: [166/282], train_loss: 0.3455, valid_loss: 0.2568, accuracy: 93.14 %\n",
            "Step: [266/282], train_loss: 0.3483, valid_loss: 0.2394, accuracy: 93.25 %\n",
            "EPOCH [88]\n",
            "Step: [84/282], train_loss: 0.3289, valid_loss: 0.2606, accuracy: 92.21 %\n",
            "Step: [184/282], train_loss: 0.3411, valid_loss: 0.2663, accuracy: 92.72 %\n",
            "EPOCH [89]\n",
            "Step: [2/282], train_loss: 0.4425, valid_loss: 0.2745, accuracy: 92.17 %\n",
            "Step: [102/282], train_loss: 0.3493, valid_loss: 0.2490, accuracy: 92.74 %\n",
            "Step: [202/282], train_loss: 0.3440, valid_loss: 0.2388, accuracy: 93.06 %\n",
            "EPOCH [90]\n",
            "Step: [20/282], train_loss: 0.3394, valid_loss: 0.2369, accuracy: 93.20 %\n",
            "Step: [120/282], train_loss: 0.3426, valid_loss: 0.2512, accuracy: 93.02 %\n",
            "Step: [220/282], train_loss: 0.3429, valid_loss: 0.2744, accuracy: 92.03 %\n",
            "EPOCH [91]\n",
            "Step: [38/282], train_loss: 0.3195, valid_loss: 0.2460, accuracy: 93.42 %\n",
            "Step: [138/282], train_loss: 0.3356, valid_loss: 0.2363, accuracy: 93.19 %\n",
            "Step: [238/282], train_loss: 0.3404, valid_loss: 0.2440, accuracy: 93.16 %\n",
            "EPOCH [92]\n",
            "Step: [56/282], train_loss: 0.3473, valid_loss: 0.2436, accuracy: 92.86 %\n",
            "Step: [156/282], train_loss: 0.3426, valid_loss: 0.2545, accuracy: 92.85 %\n",
            "Step: [256/282], train_loss: 0.3422, valid_loss: 0.2468, accuracy: 92.97 %\n",
            "EPOCH [93]\n",
            "Step: [74/282], train_loss: 0.3397, valid_loss: 0.2435, accuracy: 93.14 %\n",
            "Step: [174/282], train_loss: 0.3371, valid_loss: 0.2515, accuracy: 92.94 %\n",
            "Step: [274/282], train_loss: 0.3377, valid_loss: 0.2432, accuracy: 92.86 %\n",
            "EPOCH [94]\n",
            "Step: [92/282], train_loss: 0.3313, valid_loss: 0.2519, accuracy: 92.68 %\n",
            "Step: [192/282], train_loss: 0.3277, valid_loss: 0.2433, accuracy: 92.91 %\n",
            "EPOCH [95]\n",
            "Step: [10/282], train_loss: 0.3579, valid_loss: 0.2434, accuracy: 92.94 %\n",
            "Step: [110/282], train_loss: 0.3232, valid_loss: 0.2402, accuracy: 93.09 %\n",
            "Step: [210/282], train_loss: 0.3335, valid_loss: 0.2416, accuracy: 92.74 %\n",
            "EPOCH [96]\n",
            "Step: [28/282], train_loss: 0.3412, valid_loss: 0.2391, accuracy: 93.34 %\n",
            "Step: [128/282], train_loss: 0.3341, valid_loss: 0.2462, accuracy: 93.27 %\n",
            "Step: [228/282], train_loss: 0.3371, valid_loss: 0.2381, accuracy: 93.43 %\n",
            "EPOCH [97]\n",
            "Step: [46/282], train_loss: 0.3174, valid_loss: 0.2314, accuracy: 93.37 %\n",
            "Step: [146/282], train_loss: 0.3222, valid_loss: 0.2385, accuracy: 93.24 %\n",
            "Step: [246/282], train_loss: 0.3296, valid_loss: 0.2427, accuracy: 93.30 %\n",
            "EPOCH [98]\n",
            "Step: [64/282], train_loss: 0.3422, valid_loss: 0.2426, accuracy: 92.93 %\n",
            "Step: [164/282], train_loss: 0.3377, valid_loss: 0.2380, accuracy: 92.87 %\n",
            "Step: [264/282], train_loss: 0.3365, valid_loss: 0.2420, accuracy: 93.26 %\n",
            "EPOCH [99]\n",
            "Step: [82/282], train_loss: 0.3229, valid_loss: 0.2673, accuracy: 92.56 %\n",
            "Step: [182/282], train_loss: 0.3253, valid_loss: 0.2430, accuracy: 93.14 %\n",
            "Step: [282/282], train_loss: 0.3237, valid_loss: 0.2885, accuracy: 91.94 %\n",
            "EPOCH [100]\n",
            "Step: [100/282], train_loss: 0.3426, valid_loss: 0.2280, accuracy: 93.50 %\n",
            "Step: [200/282], train_loss: 0.3294, valid_loss: 0.2264, accuracy: 93.49 %\n",
            "EPOCH [101]\n",
            "Step: [18/282], train_loss: 0.3271, valid_loss: 0.2297, accuracy: 93.45 %\n",
            "Step: [118/282], train_loss: 0.3140, valid_loss: 0.2306, accuracy: 93.34 %\n",
            "Step: [218/282], train_loss: 0.3103, valid_loss: 0.2249, accuracy: 93.46 %\n",
            "EPOCH [102]\n",
            "Step: [36/282], train_loss: 0.2947, valid_loss: 0.2258, accuracy: 93.49 %\n",
            "Step: [136/282], train_loss: 0.3028, valid_loss: 0.2270, accuracy: 93.63 %\n",
            "Step: [236/282], train_loss: 0.3083, valid_loss: 0.2239, accuracy: 93.53 %\n",
            "EPOCH [103]\n",
            "Step: [54/282], train_loss: 0.3101, valid_loss: 0.2248, accuracy: 93.51 %\n",
            "Step: [154/282], train_loss: 0.3061, valid_loss: 0.2261, accuracy: 93.55 %\n",
            "Step: [254/282], train_loss: 0.3067, valid_loss: 0.2266, accuracy: 93.49 %\n",
            "EPOCH [104]\n",
            "Step: [72/282], train_loss: 0.3068, valid_loss: 0.2261, accuracy: 93.51 %\n",
            "Step: [172/282], train_loss: 0.3031, valid_loss: 0.2257, accuracy: 93.59 %\n",
            "Step: [272/282], train_loss: 0.3037, valid_loss: 0.2264, accuracy: 93.45 %\n",
            "EPOCH [105]\n",
            "Step: [90/282], train_loss: 0.2990, valid_loss: 0.2259, accuracy: 93.55 %\n",
            "Step: [190/282], train_loss: 0.3044, valid_loss: 0.2271, accuracy: 93.51 %\n",
            "EPOCH [106]\n",
            "Step: [8/282], train_loss: 0.2975, valid_loss: 0.2290, accuracy: 93.55 %\n",
            "Step: [108/282], train_loss: 0.3052, valid_loss: 0.2272, accuracy: 93.52 %\n",
            "Step: [208/282], train_loss: 0.3051, valid_loss: 0.2259, accuracy: 93.49 %\n",
            "EPOCH [107]\n",
            "Step: [26/282], train_loss: 0.2928, valid_loss: 0.2260, accuracy: 93.44 %\n",
            "Step: [126/282], train_loss: 0.3087, valid_loss: 0.2289, accuracy: 93.54 %\n",
            "Step: [226/282], train_loss: 0.3046, valid_loss: 0.2258, accuracy: 93.49 %\n",
            "EPOCH [108]\n",
            "Step: [44/282], train_loss: 0.3087, valid_loss: 0.2278, accuracy: 93.47 %\n",
            "Step: [144/282], train_loss: 0.3045, valid_loss: 0.2277, accuracy: 93.50 %\n",
            "Step: [244/282], train_loss: 0.3052, valid_loss: 0.2283, accuracy: 93.61 %\n",
            "EPOCH [109]\n",
            "Step: [62/282], train_loss: 0.2960, valid_loss: 0.2259, accuracy: 93.52 %\n",
            "Step: [162/282], train_loss: 0.3005, valid_loss: 0.2258, accuracy: 93.55 %\n",
            "Step: [262/282], train_loss: 0.3022, valid_loss: 0.2276, accuracy: 93.46 %\n",
            "EPOCH [110]\n",
            "Step: [80/282], train_loss: 0.3072, valid_loss: 0.2251, accuracy: 93.66 %\n",
            "Step: [180/282], train_loss: 0.3003, valid_loss: 0.2246, accuracy: 93.50 %\n",
            "Step: [280/282], train_loss: 0.3003, valid_loss: 0.2269, accuracy: 93.53 %\n",
            "EPOCH [111]\n",
            "Step: [98/282], train_loss: 0.3067, valid_loss: 0.2281, accuracy: 93.51 %\n",
            "Step: [198/282], train_loss: 0.3052, valid_loss: 0.2250, accuracy: 93.50 %\n",
            "EPOCH [112]\n",
            "Step: [16/282], train_loss: 0.3307, valid_loss: 0.2283, accuracy: 93.39 %\n",
            "Step: [116/282], train_loss: 0.3090, valid_loss: 0.2287, accuracy: 93.44 %\n",
            "Step: [216/282], train_loss: 0.3038, valid_loss: 0.2252, accuracy: 93.50 %\n",
            "EPOCH [113]\n",
            "Step: [34/282], train_loss: 0.3067, valid_loss: 0.2286, accuracy: 93.50 %\n",
            "Step: [134/282], train_loss: 0.3068, valid_loss: 0.2253, accuracy: 93.52 %\n",
            "Step: [234/282], train_loss: 0.3054, valid_loss: 0.2277, accuracy: 93.64 %\n",
            "EPOCH [114]\n",
            "Step: [52/282], train_loss: 0.3074, valid_loss: 0.2251, accuracy: 93.55 %\n",
            "Step: [152/282], train_loss: 0.3036, valid_loss: 0.2302, accuracy: 93.54 %\n",
            "Step: [252/282], train_loss: 0.2999, valid_loss: 0.2272, accuracy: 93.42 %\n",
            "EPOCH [115]\n",
            "Step: [70/282], train_loss: 0.3079, valid_loss: 0.2268, accuracy: 93.47 %\n",
            "Step: [170/282], train_loss: 0.3007, valid_loss: 0.2268, accuracy: 93.46 %\n",
            "Step: [270/282], train_loss: 0.3003, valid_loss: 0.2261, accuracy: 93.51 %\n",
            "EPOCH [116]\n",
            "Step: [88/282], train_loss: 0.3010, valid_loss: 0.2274, accuracy: 93.55 %\n",
            "Step: [188/282], train_loss: 0.3090, valid_loss: 0.2284, accuracy: 93.53 %\n",
            "EPOCH [117]\n",
            "Step: [6/282], train_loss: 0.3042, valid_loss: 0.2267, accuracy: 93.46 %\n",
            "Step: [106/282], train_loss: 0.2994, valid_loss: 0.2294, accuracy: 93.60 %\n",
            "Step: [206/282], train_loss: 0.3020, valid_loss: 0.2266, accuracy: 93.56 %\n",
            "EPOCH [118]\n",
            "Step: [24/282], train_loss: 0.3103, valid_loss: 0.2263, accuracy: 93.65 %\n",
            "Step: [124/282], train_loss: 0.3022, valid_loss: 0.2291, accuracy: 93.63 %\n",
            "Step: [224/282], train_loss: 0.3033, valid_loss: 0.2259, accuracy: 93.44 %\n",
            "EPOCH [119]\n",
            "Step: [42/282], train_loss: 0.3050, valid_loss: 0.2283, accuracy: 93.61 %\n",
            "Step: [142/282], train_loss: 0.3058, valid_loss: 0.2278, accuracy: 93.54 %\n",
            "Step: [242/282], train_loss: 0.3014, valid_loss: 0.2267, accuracy: 93.56 %\n",
            "EPOCH [120]\n",
            "Step: [60/282], train_loss: 0.2950, valid_loss: 0.2272, accuracy: 93.42 %\n",
            "Step: [160/282], train_loss: 0.2932, valid_loss: 0.2272, accuracy: 93.57 %\n",
            "Step: [260/282], train_loss: 0.2954, valid_loss: 0.2289, accuracy: 93.57 %\n",
            "EPOCH [121]\n",
            "Step: [78/282], train_loss: 0.3065, valid_loss: 0.2293, accuracy: 93.57 %\n",
            "Step: [178/282], train_loss: 0.3002, valid_loss: 0.2281, accuracy: 93.66 %\n",
            "Step: [278/282], train_loss: 0.3021, valid_loss: 0.2269, accuracy: 93.60 %\n",
            "EPOCH [122]\n",
            "Step: [96/282], train_loss: 0.2947, valid_loss: 0.2297, accuracy: 93.57 %\n",
            "Step: [196/282], train_loss: 0.2997, valid_loss: 0.2278, accuracy: 93.63 %\n",
            "EPOCH [123]\n",
            "Step: [14/282], train_loss: 0.2780, valid_loss: 0.2288, accuracy: 93.67 %\n",
            "Step: [114/282], train_loss: 0.2963, valid_loss: 0.2300, accuracy: 93.67 %\n",
            "Step: [214/282], train_loss: 0.2971, valid_loss: 0.2270, accuracy: 93.59 %\n",
            "EPOCH [124]\n",
            "Step: [32/282], train_loss: 0.3174, valid_loss: 0.2268, accuracy: 93.63 %\n",
            "Step: [132/282], train_loss: 0.3039, valid_loss: 0.2294, accuracy: 93.65 %\n",
            "Step: [232/282], train_loss: 0.3018, valid_loss: 0.2268, accuracy: 93.64 %\n",
            "EPOCH [125]\n",
            "Step: [50/282], train_loss: 0.2868, valid_loss: 0.2271, accuracy: 93.59 %\n",
            "Step: [150/282], train_loss: 0.2940, valid_loss: 0.2280, accuracy: 93.66 %\n",
            "Step: [250/282], train_loss: 0.2952, valid_loss: 0.2285, accuracy: 93.59 %\n",
            "EPOCH [126]\n",
            "Step: [68/282], train_loss: 0.2980, valid_loss: 0.2268, accuracy: 93.55 %\n",
            "Step: [168/282], train_loss: 0.3023, valid_loss: 0.2318, accuracy: 93.63 %\n",
            "Step: [268/282], train_loss: 0.2989, valid_loss: 0.2265, accuracy: 93.61 %\n",
            "EPOCH [127]\n",
            "Step: [86/282], train_loss: 0.2976, valid_loss: 0.2282, accuracy: 93.61 %\n",
            "Step: [186/282], train_loss: 0.2974, valid_loss: 0.2289, accuracy: 93.46 %\n",
            "EPOCH [128]\n",
            "Step: [4/282], train_loss: 0.2694, valid_loss: 0.2276, accuracy: 93.57 %\n",
            "Step: [104/282], train_loss: 0.2995, valid_loss: 0.2282, accuracy: 93.63 %\n",
            "Step: [204/282], train_loss: 0.3031, valid_loss: 0.2255, accuracy: 93.57 %\n",
            "EPOCH [129]\n",
            "Step: [22/282], train_loss: 0.2990, valid_loss: 0.2274, accuracy: 93.65 %\n",
            "Step: [122/282], train_loss: 0.3052, valid_loss: 0.2275, accuracy: 93.63 %\n",
            "Step: [222/282], train_loss: 0.3019, valid_loss: 0.2291, accuracy: 93.52 %\n",
            "EPOCH [130]\n",
            "Step: [40/282], train_loss: 0.3160, valid_loss: 0.2275, accuracy: 93.57 %\n",
            "Step: [140/282], train_loss: 0.3010, valid_loss: 0.2262, accuracy: 93.64 %\n",
            "Step: [240/282], train_loss: 0.2997, valid_loss: 0.2266, accuracy: 93.65 %\n",
            "EPOCH [131]\n",
            "Step: [58/282], train_loss: 0.3074, valid_loss: 0.2305, accuracy: 93.67 %\n",
            "Step: [158/282], train_loss: 0.3022, valid_loss: 0.2284, accuracy: 93.66 %\n",
            "Step: [258/282], train_loss: 0.3011, valid_loss: 0.2278, accuracy: 93.64 %\n",
            "EPOCH [132]\n",
            "Step: [76/282], train_loss: 0.3081, valid_loss: 0.2303, accuracy: 93.64 %\n",
            "Step: [176/282], train_loss: 0.3067, valid_loss: 0.2273, accuracy: 93.62 %\n",
            "Step: [276/282], train_loss: 0.3037, valid_loss: 0.2284, accuracy: 93.61 %\n",
            "EPOCH [133]\n",
            "Step: [94/282], train_loss: 0.2886, valid_loss: 0.2277, accuracy: 93.65 %\n",
            "Step: [194/282], train_loss: 0.2914, valid_loss: 0.2287, accuracy: 93.63 %\n",
            "EPOCH [134]\n",
            "Step: [12/282], train_loss: 0.2755, valid_loss: 0.2279, accuracy: 93.65 %\n",
            "Step: [112/282], train_loss: 0.3029, valid_loss: 0.2301, accuracy: 93.69 %\n",
            "Step: [212/282], train_loss: 0.2996, valid_loss: 0.2290, accuracy: 93.62 %\n",
            "EPOCH [135]\n",
            "Step: [30/282], train_loss: 0.2987, valid_loss: 0.2315, accuracy: 93.60 %\n",
            "Step: [130/282], train_loss: 0.2966, valid_loss: 0.2278, accuracy: 93.57 %\n",
            "Step: [230/282], train_loss: 0.2991, valid_loss: 0.2298, accuracy: 93.62 %\n",
            "EPOCH [136]\n",
            "Step: [48/282], train_loss: 0.2784, valid_loss: 0.2291, accuracy: 93.61 %\n",
            "Step: [148/282], train_loss: 0.2880, valid_loss: 0.2323, accuracy: 93.61 %\n",
            "Step: [248/282], train_loss: 0.2954, valid_loss: 0.2287, accuracy: 93.61 %\n",
            "EPOCH [137]\n",
            "Step: [66/282], train_loss: 0.3161, valid_loss: 0.2286, accuracy: 93.64 %\n",
            "Step: [166/282], train_loss: 0.3087, valid_loss: 0.2297, accuracy: 93.56 %\n",
            "Step: [266/282], train_loss: 0.3056, valid_loss: 0.2286, accuracy: 93.60 %\n",
            "EPOCH [138]\n",
            "Step: [84/282], train_loss: 0.2954, valid_loss: 0.2286, accuracy: 93.57 %\n",
            "Step: [184/282], train_loss: 0.3000, valid_loss: 0.2283, accuracy: 93.65 %\n",
            "EPOCH [139]\n",
            "Step: [2/282], train_loss: 0.3045, valid_loss: 0.2292, accuracy: 93.50 %\n",
            "Step: [102/282], train_loss: 0.2983, valid_loss: 0.2296, accuracy: 93.59 %\n",
            "Step: [202/282], train_loss: 0.2975, valid_loss: 0.2308, accuracy: 93.57 %\n",
            "EPOCH [140]\n",
            "Step: [20/282], train_loss: 0.2967, valid_loss: 0.2301, accuracy: 93.56 %\n",
            "Step: [120/282], train_loss: 0.2928, valid_loss: 0.2290, accuracy: 93.56 %\n",
            "Step: [220/282], train_loss: 0.2940, valid_loss: 0.2291, accuracy: 93.61 %\n",
            "EPOCH [141]\n",
            "Step: [38/282], train_loss: 0.2852, valid_loss: 0.2273, accuracy: 93.56 %\n",
            "Step: [138/282], train_loss: 0.2943, valid_loss: 0.2297, accuracy: 93.64 %\n",
            "Step: [238/282], train_loss: 0.2970, valid_loss: 0.2293, accuracy: 93.56 %\n",
            "EPOCH [142]\n",
            "Step: [56/282], train_loss: 0.3033, valid_loss: 0.2288, accuracy: 93.46 %\n",
            "Step: [156/282], train_loss: 0.2958, valid_loss: 0.2286, accuracy: 93.64 %\n",
            "Step: [256/282], train_loss: 0.2964, valid_loss: 0.2306, accuracy: 93.56 %\n",
            "EPOCH [143]\n",
            "Step: [74/282], train_loss: 0.2914, valid_loss: 0.2292, accuracy: 93.54 %\n",
            "Step: [174/282], train_loss: 0.2920, valid_loss: 0.2312, accuracy: 93.55 %\n",
            "Step: [274/282], train_loss: 0.2949, valid_loss: 0.2314, accuracy: 93.57 %\n",
            "EPOCH [144]\n",
            "Step: [92/282], train_loss: 0.2980, valid_loss: 0.2306, accuracy: 93.59 %\n",
            "Step: [192/282], train_loss: 0.2979, valid_loss: 0.2312, accuracy: 93.54 %\n",
            "EPOCH [145]\n",
            "Step: [10/282], train_loss: 0.2732, valid_loss: 0.2320, accuracy: 93.61 %\n",
            "Step: [110/282], train_loss: 0.3036, valid_loss: 0.2302, accuracy: 93.59 %\n",
            "Step: [210/282], train_loss: 0.3000, valid_loss: 0.2310, accuracy: 93.62 %\n",
            "EPOCH [146]\n",
            "Step: [28/282], train_loss: 0.2889, valid_loss: 0.2307, accuracy: 93.57 %\n",
            "Step: [128/282], train_loss: 0.3021, valid_loss: 0.2309, accuracy: 93.64 %\n",
            "Step: [228/282], train_loss: 0.2947, valid_loss: 0.2316, accuracy: 93.64 %\n",
            "EPOCH [147]\n",
            "Step: [46/282], train_loss: 0.3029, valid_loss: 0.2329, accuracy: 93.66 %\n",
            "Step: [146/282], train_loss: 0.3039, valid_loss: 0.2314, accuracy: 93.57 %\n",
            "Step: [246/282], train_loss: 0.2988, valid_loss: 0.2312, accuracy: 93.54 %\n",
            "EPOCH [148]\n",
            "Step: [64/282], train_loss: 0.2846, valid_loss: 0.2320, accuracy: 93.59 %\n",
            "Step: [164/282], train_loss: 0.2886, valid_loss: 0.2322, accuracy: 93.59 %\n",
            "Step: [264/282], train_loss: 0.2915, valid_loss: 0.2335, accuracy: 93.59 %\n",
            "EPOCH [149]\n",
            "Step: [82/282], train_loss: 0.2920, valid_loss: 0.2331, accuracy: 93.59 %\n",
            "Step: [182/282], train_loss: 0.2867, valid_loss: 0.2328, accuracy: 93.55 %\n",
            "Step: [282/282], train_loss: 0.2867, valid_loss: 0.2305, accuracy: 93.57 %\n",
            "EPOCH [150]\n",
            "Step: [100/282], train_loss: 0.2871, valid_loss: 0.2313, accuracy: 93.63 %\n",
            "Step: [200/282], train_loss: 0.2833, valid_loss: 0.2314, accuracy: 93.59 %\n",
            "EPOCH [151]\n",
            "Step: [18/282], train_loss: 0.2855, valid_loss: 0.2311, accuracy: 93.55 %\n",
            "Step: [118/282], train_loss: 0.2873, valid_loss: 0.2319, accuracy: 93.61 %\n",
            "Step: [218/282], train_loss: 0.2867, valid_loss: 0.2320, accuracy: 93.62 %\n",
            "EPOCH [152]\n",
            "Step: [36/282], train_loss: 0.2961, valid_loss: 0.2316, accuracy: 93.59 %\n",
            "Step: [136/282], train_loss: 0.2886, valid_loss: 0.2316, accuracy: 93.60 %\n",
            "Step: [236/282], train_loss: 0.2882, valid_loss: 0.2316, accuracy: 93.62 %\n",
            "EPOCH [153]\n",
            "Step: [54/282], train_loss: 0.2992, valid_loss: 0.2317, accuracy: 93.59 %\n",
            "Step: [154/282], train_loss: 0.2941, valid_loss: 0.2322, accuracy: 93.57 %\n",
            "Step: [254/282], train_loss: 0.2892, valid_loss: 0.2319, accuracy: 93.55 %\n",
            "EPOCH [154]\n",
            "Step: [72/282], train_loss: 0.2724, valid_loss: 0.2322, accuracy: 93.57 %\n",
            "Step: [172/282], train_loss: 0.2839, valid_loss: 0.2321, accuracy: 93.56 %\n",
            "Step: [272/282], train_loss: 0.2877, valid_loss: 0.2320, accuracy: 93.56 %\n",
            "EPOCH [155]\n",
            "Step: [90/282], train_loss: 0.2798, valid_loss: 0.2321, accuracy: 93.60 %\n",
            "Step: [190/282], train_loss: 0.2892, valid_loss: 0.2322, accuracy: 93.57 %\n",
            "EPOCH [156]\n",
            "Step: [8/282], train_loss: 0.2727, valid_loss: 0.2320, accuracy: 93.55 %\n",
            "Step: [108/282], train_loss: 0.2903, valid_loss: 0.2320, accuracy: 93.56 %\n",
            "Step: [208/282], train_loss: 0.2862, valid_loss: 0.2315, accuracy: 93.52 %\n",
            "EPOCH [157]\n",
            "Step: [26/282], train_loss: 0.2682, valid_loss: 0.2326, accuracy: 93.56 %\n",
            "Step: [126/282], train_loss: 0.2897, valid_loss: 0.2326, accuracy: 93.63 %\n",
            "Step: [226/282], train_loss: 0.2890, valid_loss: 0.2322, accuracy: 93.54 %\n",
            "EPOCH [158]\n",
            "Step: [44/282], train_loss: 0.2855, valid_loss: 0.2325, accuracy: 93.60 %\n",
            "Step: [144/282], train_loss: 0.2906, valid_loss: 0.2319, accuracy: 93.56 %\n",
            "Step: [244/282], train_loss: 0.2872, valid_loss: 0.2323, accuracy: 93.61 %\n",
            "EPOCH [159]\n",
            "Step: [62/282], train_loss: 0.2831, valid_loss: 0.2324, accuracy: 93.57 %\n",
            "Step: [162/282], train_loss: 0.2877, valid_loss: 0.2328, accuracy: 93.60 %\n",
            "Step: [262/282], train_loss: 0.2846, valid_loss: 0.2321, accuracy: 93.57 %\n",
            "EPOCH [160]\n",
            "Step: [80/282], train_loss: 0.2939, valid_loss: 0.2321, accuracy: 93.61 %\n",
            "Step: [180/282], train_loss: 0.2947, valid_loss: 0.2326, accuracy: 93.61 %\n",
            "Step: [280/282], train_loss: 0.2857, valid_loss: 0.2323, accuracy: 93.57 %\n",
            "EPOCH [161]\n",
            "Step: [98/282], train_loss: 0.2852, valid_loss: 0.2323, accuracy: 93.60 %\n",
            "Step: [198/282], train_loss: 0.2856, valid_loss: 0.2321, accuracy: 93.54 %\n",
            "EPOCH [162]\n",
            "Step: [16/282], train_loss: 0.2761, valid_loss: 0.2324, accuracy: 93.57 %\n",
            "Step: [116/282], train_loss: 0.2829, valid_loss: 0.2321, accuracy: 93.56 %\n",
            "Step: [216/282], train_loss: 0.2872, valid_loss: 0.2322, accuracy: 93.64 %\n",
            "EPOCH [163]\n",
            "Step: [34/282], train_loss: 0.2770, valid_loss: 0.2324, accuracy: 93.56 %\n",
            "Step: [134/282], train_loss: 0.2859, valid_loss: 0.2324, accuracy: 93.54 %\n",
            "Step: [234/282], train_loss: 0.2910, valid_loss: 0.2326, accuracy: 93.52 %\n",
            "EPOCH [164]\n",
            "Step: [52/282], train_loss: 0.2922, valid_loss: 0.2325, accuracy: 93.54 %\n",
            "Step: [152/282], train_loss: 0.2938, valid_loss: 0.2326, accuracy: 93.55 %\n",
            "Step: [252/282], train_loss: 0.2909, valid_loss: 0.2326, accuracy: 93.59 %\n",
            "EPOCH [165]\n",
            "Step: [70/282], train_loss: 0.2889, valid_loss: 0.2325, accuracy: 93.63 %\n",
            "Step: [170/282], train_loss: 0.2869, valid_loss: 0.2324, accuracy: 93.60 %\n",
            "Step: [270/282], train_loss: 0.2873, valid_loss: 0.2327, accuracy: 93.59 %\n",
            "EPOCH [166]\n",
            "Step: [88/282], train_loss: 0.2877, valid_loss: 0.2322, accuracy: 93.55 %\n",
            "Step: [188/282], train_loss: 0.2886, valid_loss: 0.2326, accuracy: 93.55 %\n",
            "EPOCH [167]\n",
            "Step: [6/282], train_loss: 0.2718, valid_loss: 0.2330, accuracy: 93.55 %\n",
            "Step: [106/282], train_loss: 0.2979, valid_loss: 0.2328, accuracy: 93.54 %\n",
            "Step: [206/282], train_loss: 0.2907, valid_loss: 0.2323, accuracy: 93.56 %\n",
            "EPOCH [168]\n",
            "Step: [24/282], train_loss: 0.3062, valid_loss: 0.2325, accuracy: 93.60 %\n",
            "Step: [124/282], train_loss: 0.2896, valid_loss: 0.2326, accuracy: 93.60 %\n",
            "Step: [224/282], train_loss: 0.2852, valid_loss: 0.2324, accuracy: 93.55 %\n",
            "EPOCH [169]\n",
            "Step: [42/282], train_loss: 0.3022, valid_loss: 0.2331, accuracy: 93.54 %\n",
            "Step: [142/282], train_loss: 0.2888, valid_loss: 0.2330, accuracy: 93.57 %\n",
            "Step: [242/282], train_loss: 0.2879, valid_loss: 0.2330, accuracy: 93.54 %\n",
            "EPOCH [170]\n",
            "Step: [60/282], train_loss: 0.2843, valid_loss: 0.2328, accuracy: 93.61 %\n",
            "Step: [160/282], train_loss: 0.2919, valid_loss: 0.2329, accuracy: 93.62 %\n",
            "Step: [260/282], train_loss: 0.2894, valid_loss: 0.2328, accuracy: 93.55 %\n",
            "EPOCH [171]\n",
            "Step: [78/282], train_loss: 0.2794, valid_loss: 0.2329, accuracy: 93.55 %\n",
            "Step: [178/282], train_loss: 0.2834, valid_loss: 0.2326, accuracy: 93.55 %\n",
            "Step: [278/282], train_loss: 0.2886, valid_loss: 0.2328, accuracy: 93.55 %\n",
            "EPOCH [172]\n",
            "Step: [96/282], train_loss: 0.2810, valid_loss: 0.2333, accuracy: 93.60 %\n",
            "Step: [196/282], train_loss: 0.2877, valid_loss: 0.2333, accuracy: 93.61 %\n",
            "EPOCH [173]\n",
            "Step: [14/282], train_loss: 0.2438, valid_loss: 0.2328, accuracy: 93.61 %\n",
            "Step: [114/282], train_loss: 0.2858, valid_loss: 0.2332, accuracy: 93.61 %\n",
            "Step: [214/282], train_loss: 0.2842, valid_loss: 0.2331, accuracy: 93.59 %\n",
            "EPOCH [174]\n",
            "Step: [32/282], train_loss: 0.2904, valid_loss: 0.2333, accuracy: 93.60 %\n",
            "Step: [132/282], train_loss: 0.2885, valid_loss: 0.2337, accuracy: 93.57 %\n",
            "Step: [232/282], train_loss: 0.2873, valid_loss: 0.2328, accuracy: 93.59 %\n",
            "EPOCH [175]\n",
            "Step: [50/282], train_loss: 0.3060, valid_loss: 0.2327, accuracy: 93.61 %\n",
            "Step: [150/282], train_loss: 0.2913, valid_loss: 0.2328, accuracy: 93.59 %\n",
            "Step: [250/282], train_loss: 0.2863, valid_loss: 0.2329, accuracy: 93.59 %\n",
            "EPOCH [176]\n",
            "Step: [68/282], train_loss: 0.2866, valid_loss: 0.2330, accuracy: 93.62 %\n",
            "Step: [168/282], train_loss: 0.2935, valid_loss: 0.2329, accuracy: 93.56 %\n",
            "Step: [268/282], train_loss: 0.2912, valid_loss: 0.2334, accuracy: 93.60 %\n",
            "EPOCH [177]\n",
            "Step: [86/282], train_loss: 0.2798, valid_loss: 0.2326, accuracy: 93.62 %\n",
            "Step: [186/282], train_loss: 0.2842, valid_loss: 0.2333, accuracy: 93.56 %\n",
            "EPOCH [178]\n",
            "Step: [4/282], train_loss: 0.3277, valid_loss: 0.2336, accuracy: 93.59 %\n",
            "Step: [104/282], train_loss: 0.2816, valid_loss: 0.2340, accuracy: 93.57 %\n",
            "Step: [204/282], train_loss: 0.2862, valid_loss: 0.2331, accuracy: 93.57 %\n",
            "EPOCH [179]\n",
            "Step: [22/282], train_loss: 0.2740, valid_loss: 0.2336, accuracy: 93.60 %\n",
            "Step: [122/282], train_loss: 0.2824, valid_loss: 0.2333, accuracy: 93.57 %\n",
            "Step: [222/282], train_loss: 0.2881, valid_loss: 0.2326, accuracy: 93.56 %\n",
            "EPOCH [180]\n",
            "Step: [40/282], train_loss: 0.2808, valid_loss: 0.2326, accuracy: 93.55 %\n",
            "Step: [140/282], train_loss: 0.2851, valid_loss: 0.2326, accuracy: 93.56 %\n",
            "Step: [240/282], train_loss: 0.2787, valid_loss: 0.2322, accuracy: 93.59 %\n",
            "EPOCH [181]\n",
            "Step: [58/282], train_loss: 0.2987, valid_loss: 0.2326, accuracy: 93.57 %\n",
            "Step: [158/282], train_loss: 0.2877, valid_loss: 0.2328, accuracy: 93.60 %\n",
            "Step: [258/282], train_loss: 0.2889, valid_loss: 0.2331, accuracy: 93.61 %\n",
            "EPOCH [182]\n",
            "Step: [76/282], train_loss: 0.2796, valid_loss: 0.2332, accuracy: 93.60 %\n",
            "Step: [176/282], train_loss: 0.2868, valid_loss: 0.2325, accuracy: 93.59 %\n",
            "Step: [276/282], train_loss: 0.2847, valid_loss: 0.2327, accuracy: 93.56 %\n",
            "EPOCH [183]\n",
            "Step: [94/282], train_loss: 0.2843, valid_loss: 0.2332, accuracy: 93.55 %\n",
            "Step: [194/282], train_loss: 0.2874, valid_loss: 0.2330, accuracy: 93.59 %\n",
            "EPOCH [184]\n",
            "Step: [12/282], train_loss: 0.3089, valid_loss: 0.2330, accuracy: 93.60 %\n",
            "Step: [112/282], train_loss: 0.2981, valid_loss: 0.2326, accuracy: 93.60 %\n",
            "Step: [212/282], train_loss: 0.2922, valid_loss: 0.2323, accuracy: 93.59 %\n",
            "EPOCH [185]\n",
            "Step: [30/282], train_loss: 0.2950, valid_loss: 0.2329, accuracy: 93.62 %\n",
            "Step: [130/282], train_loss: 0.2862, valid_loss: 0.2325, accuracy: 93.59 %\n",
            "Step: [230/282], train_loss: 0.2864, valid_loss: 0.2329, accuracy: 93.62 %\n",
            "EPOCH [186]\n",
            "Step: [48/282], train_loss: 0.2764, valid_loss: 0.2331, accuracy: 93.57 %\n",
            "Step: [148/282], train_loss: 0.2827, valid_loss: 0.2323, accuracy: 93.59 %\n",
            "Step: [248/282], train_loss: 0.2851, valid_loss: 0.2330, accuracy: 93.60 %\n",
            "EPOCH [187]\n",
            "Step: [66/282], train_loss: 0.2863, valid_loss: 0.2332, accuracy: 93.61 %\n",
            "Step: [166/282], train_loss: 0.2860, valid_loss: 0.2331, accuracy: 93.64 %\n",
            "Step: [266/282], train_loss: 0.2866, valid_loss: 0.2333, accuracy: 93.61 %\n",
            "EPOCH [188]\n",
            "Step: [84/282], train_loss: 0.2833, valid_loss: 0.2334, accuracy: 93.61 %\n",
            "Step: [184/282], train_loss: 0.2909, valid_loss: 0.2333, accuracy: 93.63 %\n",
            "EPOCH [189]\n",
            "Step: [2/282], train_loss: 0.4028, valid_loss: 0.2324, accuracy: 93.60 %\n",
            "Step: [102/282], train_loss: 0.2926, valid_loss: 0.2326, accuracy: 93.60 %\n",
            "Step: [202/282], train_loss: 0.2916, valid_loss: 0.2323, accuracy: 93.60 %\n",
            "EPOCH [190]\n",
            "Step: [20/282], train_loss: 0.2861, valid_loss: 0.2326, accuracy: 93.60 %\n",
            "Step: [120/282], train_loss: 0.2876, valid_loss: 0.2327, accuracy: 93.61 %\n",
            "Step: [220/282], train_loss: 0.2862, valid_loss: 0.2332, accuracy: 93.63 %\n",
            "EPOCH [191]\n",
            "Step: [38/282], train_loss: 0.2850, valid_loss: 0.2326, accuracy: 93.63 %\n",
            "Step: [138/282], train_loss: 0.2824, valid_loss: 0.2326, accuracy: 93.60 %\n",
            "Step: [238/282], train_loss: 0.2848, valid_loss: 0.2327, accuracy: 93.57 %\n",
            "EPOCH [192]\n",
            "Step: [56/282], train_loss: 0.2841, valid_loss: 0.2328, accuracy: 93.63 %\n",
            "Step: [156/282], train_loss: 0.2866, valid_loss: 0.2334, accuracy: 93.66 %\n",
            "Step: [256/282], train_loss: 0.2852, valid_loss: 0.2328, accuracy: 93.64 %\n",
            "EPOCH [193]\n",
            "Step: [74/282], train_loss: 0.2825, valid_loss: 0.2328, accuracy: 93.56 %\n",
            "Step: [174/282], train_loss: 0.2827, valid_loss: 0.2328, accuracy: 93.62 %\n",
            "Step: [274/282], train_loss: 0.2863, valid_loss: 0.2333, accuracy: 93.65 %\n",
            "EPOCH [194]\n",
            "Step: [92/282], train_loss: 0.2951, valid_loss: 0.2332, accuracy: 93.64 %\n",
            "Step: [192/282], train_loss: 0.2880, valid_loss: 0.2330, accuracy: 93.62 %\n",
            "EPOCH [195]\n",
            "Step: [10/282], train_loss: 0.2740, valid_loss: 0.2331, accuracy: 93.63 %\n",
            "Step: [110/282], train_loss: 0.2774, valid_loss: 0.2327, accuracy: 93.56 %\n",
            "Step: [210/282], train_loss: 0.2836, valid_loss: 0.2332, accuracy: 93.65 %\n",
            "EPOCH [196]\n",
            "Step: [28/282], train_loss: 0.2635, valid_loss: 0.2331, accuracy: 93.60 %\n",
            "Step: [128/282], train_loss: 0.2905, valid_loss: 0.2333, accuracy: 93.65 %\n",
            "Step: [228/282], train_loss: 0.2909, valid_loss: 0.2332, accuracy: 93.64 %\n",
            "EPOCH [197]\n",
            "Step: [46/282], train_loss: 0.2928, valid_loss: 0.2333, accuracy: 93.60 %\n",
            "Step: [146/282], train_loss: 0.2865, valid_loss: 0.2332, accuracy: 93.63 %\n",
            "Step: [246/282], train_loss: 0.2861, valid_loss: 0.2330, accuracy: 93.61 %\n",
            "EPOCH [198]\n",
            "Step: [64/282], train_loss: 0.2858, valid_loss: 0.2334, accuracy: 93.62 %\n",
            "Step: [164/282], train_loss: 0.2834, valid_loss: 0.2332, accuracy: 93.62 %\n",
            "Step: [264/282], train_loss: 0.2829, valid_loss: 0.2329, accuracy: 93.63 %\n",
            "EPOCH [199]\n",
            "Step: [82/282], train_loss: 0.2864, valid_loss: 0.2332, accuracy: 93.61 %\n",
            "Step: [182/282], train_loss: 0.2870, valid_loss: 0.2331, accuracy: 93.59 %\n",
            "Step: [282/282], train_loss: 0.2863, valid_loss: 0.2336, accuracy: 93.56 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3jgKuW9vFEtR"
      },
      "source": [
        "## Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W-aMsKji6iAX"
      },
      "source": [
        "testloader = DataLoader(SimpleDataset(testset), batch_size=256)\n",
        "\n",
        "right = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for samples, labels in testloader:\n",
        "\n",
        "        for key in samples:\n",
        "            samples[key] = samples[key].to('cuda')\n",
        "\n",
        "        labels = labels.to('cuda')\n",
        "\n",
        "        y_pred = classifier(**samples)\n",
        "\n",
        "        total += len(labels)\n",
        "        right += (y_pred.argmax(axis=-1) == labels).sum().item()\n",
        "\n",
        "    accuracy = right / total"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q33yzfxsHtfy",
        "outputId": "440c67f1-40ac-4262-bde0-f55866005a5c"
      },
      "source": [
        "print(\"ACCURACY: {:.2%}\".format(accuracy))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ACCURACY: 87.60%\n"
          ]
        }
      ]
    }
  ]
}